\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amsxtra,amssymb,latexsym, amscd,amsthm,eufrak, amsfonts, mathrsfs, bm, algorithm, pseudocode}
% \usepackage[pdftex]{color,graphicx}
\usepackage{pgf,tikz}
\usepackage[T5,T1]{fontenc}
\usepackage[T5]{fontenc}
\usepackage{cleveref}
\usetikzlibrary{arrows}
\usepackage{graphicx}
%\usepackage[]{algorithm2e}
%\usepackage{eucal}
\usepackage[left=3cm,right=2.3cm,top=2.5cm,bottom=2.5cm]{geometry}
%\usepackage{program}
\usepackage[all]{xy}
\UseComputerModernTips

 \usepackage[nottoc]{tocbibind}
\newcommand*\DNA{\textsc{dna}}



\newcommand{\be}{\begin{equation}}
\newcommand{\ee}{\end{equation}}
\newcommand{\beq}{\begin{eqnarray}}
\newcommand{\eeq}{\end{eqnarray}}
\newcommand{\beqn}{\begin{eqnarray*}}
\newcommand{\eeqn}{\end{eqnarray*}}
\newcommand{\parag}{\bigskip\noindent}

\newtheorem{pbm}{Problem}
\newtheorem{Property}{Property}
\numberwithin{Property}{section}
\newtheorem{Theorem}{Theorem}%[section]
\numberwithin{Theorem}{section}
\newtheorem{Proposition}{Proposition}%[section]
\numberwithin{Proposition}{section}
\newtheorem{Lemma}{Lemma}%[section]
\numberwithin{Lemma}{section}
\newtheorem{Corollary}{Corollary}%[section]
\numberwithin{Corollary}{section}
\newtheorem{Definition}{Definition}%[section]
\numberwithin{Definition}{section}
\newtheorem{Remark}{Remark}%[section]
\numberwithin{Remark}{section}
\newtheorem{Conjecture}{Conjecture}%[section]
\numberwithin{Conjecture}{section}
\newtheorem{Problem}{Problem}%[section]
\numberwithin{Problem}{section}
%\newtheorem{Example}{Example}%[section]
%\numberwithin{Example}{section}
\newtheorem{Claim}{Claim}
\numberwithin{Claim}{section}
\newtheorem{Question}{Question}%[section]
\newtheorem{Subsec}[Theorem]{}
\newtheorem{Note}[Theorem]{}
\renewcommand{\theTheorem}{\arabic{section}.\arabic{Theorem}}

\theoremstyle{definition}
\newtheorem{Example}{Example}%[section]
\numberwithin{Example}{section}

\def\bell{\mbox{\boldmath$\ell$}}
\def\mult{\ensuremath{\mathrm{mult}}}
\def\y {\ensuremath{\mathbf{y}}}
\def\e {\ensuremath{\mathbf{e}}}
\def\a {\ensuremath{\mathbf{a}}}
\def\b {\ensuremath{\mathbf{b}}}
\def\z {\ensuremath{\mathbf{z}}}
\def\w {\ensuremath{\mathbf{w}}}
\def\f {\ensuremath{\mathbf{f}}}
\def\r {\ensuremath{\mathbf{r}}}
\def\s {\ensuremath{\mathbf{s}}}
\def\L {\ensuremath{\mathbf{L}}}
%\def\F {\ensuremath{\mathbf{F}}}
\def\G {\ensuremath{\mathbf{G}}}
\def\E {\ensuremath{\mathbf{E}}}
\def\X {\ensuremath{\mathbf{X}}}
\def\Y {\ensuremath{\mathbf{Y}}}
\def\H {\ensuremath{\mathbf{H}}}
\def\bfA {\ensuremath{\mathbf{A}}}
\def\m {\ensuremath{\mathfrak{m}}}
\def\v {\ensuremath{\mathbf{v}}}
\def\u {\ensuremath{\mathbf{u}}}
\def\q {\ensuremath{\mathbf{q}}}
\def\U {\ensuremath{\mathbf{U}}}
%\def\V {\ensuremath{\mathbf{V}}}
\def\t {\ensuremath{\mathbf{t}}}
\def\h {\ensuremath{\mathbf{h}}}
\def\g {\ensuremath{\mathbf{g}}}
\newcommand{\algoname}[1]{{\normalfont\textsc{#1}}}

\newcommand{\problemname}[1]{{\normalfont\textsc{#1}}}

\newcommand{\algoword}[1]{\emph{\textsf{#1}}}

\newcommand{\assign}{\leftarrow}

\newcommand{\inlcomment}[1]{\texttt{\small/* #1 */}}

\newcommand{\eolcomment}[1]{\hfill\texttt{\small// #1}}

\renewcommand{\leq}{\leqslant} 
\renewcommand{\geq}{\geqslant} 
\renewcommand{\le}{\leqslant} 
\renewcommand{\ge}{\geqslant} 
\newcommand{\R}{{\Bbb R}}
\newcommand{\Z}{{\Bbb Z}}
\newcommand{\F}{{\Bbb F}}
\newcommand{\Fd}{\F_2}
\newcommand{\cala}{{\cal A}}
\newcommand{\V}{{\Bbb V}}
\newcommand{\glk}{GL_k}
\newcommand{\glki}{GL_{k+i}}
\newcommand{\glone}{GL_1}
\newcommand{\glf}{GL_4}
\newcommand{\glfive}{GL_5}
\newcommand{\pk}{P_k}
\newcommand{\otiglk}{\rlap{$\,\,\otimes$}\lower 7pt\hbox{$_{\glk}$}}
\newcommand{\otiglone}{\rlap{$\,\,\otimes$}\lower 7pt\hbox{$_{\glone}$}}
\newcommand{\otiglf}{\rlap{$\,\,\otimes$}\lower 7pt\hbox{$_{\glf}$}}
\newcommand{\otiglfive}{\rlap{$\,\,\otimes$}\lower 7pt\hbox{$_{\glfive}
$}}
\newcommand{\otiglki}{\rlap{$\,\,\otimes$}\lower 7pt\hbox{$_{\glki}$}}
\newcommand{\otigl}{\rlap{$\,\,\otimes$}\lower 7pt\hbox{$\,_{GL}$}}
\newcommand{\otigln}[1]{\rlap{$\,\,\otimes$}\lower 7pt\hbox{$\,_{GL_
{#1}}$}}
\newcommand{\otiTk}{\rlap{$\otimes$}\lower 7pt\hbox{$_{T_k}$}}
\newcommand{\oticala}{\,\rlap{$\otimes$}\lower 7pt\hbox{$_{\cala}$}\,}
\newcommand{\ov}{\overline}
%-------------------------


%vsize=21.1 truecm
\parskip 1pt

\font\inh=cmr10 \font\cto=cmr10
\font\tit=cmbx12 \font\tmd=cmbx10 \font\ab=cmti9 \font\cn=cmr9
\def\ker{\text{Ker}}
\def\bar{\overline}
\def\a.s{\text{\;a.s.\;}}
\def\supp{\text{supp\,}}
\pagestyle{plain}
%\font\fhead= vncentb at 10pt %font for page heading
%\font\fpart=vncoop at 30pt %font for part mark
\newlength{\tdtrai}
\newlength{\tdphai}
\newlength{\kctdtrai}
\newlength{\kctdphai}
\newcommand\automakeheading [2]{%
\def\trai{Chapter \thechapter. #1}
\def\phai{\thesection. #2}
\markboth{\trai}{\phai}}

\newcommand\manualmakeheading [2]{%
\def\trai{#1}
\def\phai{#2}
\markboth{\trai}{\phai}}
%End For heading

\setcounter{secnumdepth}{2}
\setcounter{tocdepth}{2}

%-----------------------------------------------
\def\eex{{\accent"5E e}\kern-.470em\raise.3ex\hbox{\char'176}}
\def\uw{u\kern-.44em\raise.82ex\hbox{ \vrule width .12em height .0ex depth .075ex \kern-0.16em \char'56}\kern-.05em}
\def\EEX{{\accent"5E E}\kern-.60em\raise.9ex\hbox{\char'176}\kern.1em}
\def\UW{U\kern-.42em\raise1.36ex\hbox{
\vrule width .13em height .0ex depth .075ex \kern-0.16em
\char'56}\kern-.07em}
\def\aah{{\accent"5E a}\kern-.62em\raise.2ex\hbox{\char'22}\kern.12em}

%% --------------------------------------
%% -------------- MISC ------------------
%% --------------------------------------
\newcommand{\storeArg}{} % aux, not to be used in document!!
\newcounter{notationCounter}

%% --------------------------------------
%% ----------- COST BOUNDS --------------
%% --------------------------------------
\newcommand{\bigO}[1]{\mathcal{O}(#1)} % big O for complexity
\newcommand{\softO}[1]{\mathcal{O}\tilde{~}(#1)} % soft O for complexity
\newcommand{\polmultime}[1]{\mathsf{M}(#1)}
\newcommand{\polmatmultime}[1]{\mathsf{MM}(#1)}
\newcommand{\expmatmul}{\omega} % exponent for the cost of matrix multiplication
%% sub item
\renewcommand{\labelenumii}{\theenumii}
\renewcommand{\theenumii}{\theenumi.\arabic{enumii}.}
%% --------------------------------------
%% ------------- INTEGERS ---------------
%% --------------------------------------
\renewcommand{\ge}{\geqslant} % greater or equal
\renewcommand{\le}{\leqslant} % lesser or equal
\newcommand{\ZZ}{\mathbb{Z}} % relative integers
\newcommand{\NN}{\mathbb{N}} % integers
\newcommand{\ZZp}{\mathbb{Z}_{> 0}} % positive integers
\newcommand{\NNp}{\mathbb{N}_{> 0}} % positive integers
\newcommand{\tuple}[1]{\mathbf{#1}} % tuples (mainly used for integers I think)

%% --------------------------------------
%% -------------- SPACES ----------------
%% --------------------------------------
\newcommand{\var}{X} % default variable for univariate polynomials
\newcommand{\field}{\mathbb{K}} % base field
\newcommand{\polRing}{\field[\var]} % polynomial ring
\newcommand{\module}{\mathcal{M}} % some module
\newcommand{\rdim}{m} % default row dimension
\newcommand{\cdim}{n} % default column dimension
\newcommand{\matSpace}[1][\rdim]{\renewcommand\storeArg{#1}\matSpaceAux} % scalar matrix space, 2 opt args
\newcommand{\polMatSpace}[1][\rdim]{\renewcommand\storeArg{#1}\polMatSpaceAux} % polynomial matrix space, 2 opt args
\newcommand{\matSpaceAux}[1][\storeArg]{\field^{\storeArg \times #1}} % not to be used in document
\newcommand{\polMatSpaceAux}[1][\storeArg]{\polRing^{\storeArg \times #1}} % not to be used in document

%% --------------------------------------
%% ----------- SETS,ELEMENTS ------------
%% --------------------------------------
\newcommand{\row}[1]{\mathbf{\MakeLowercase{#1}}} % for a row of a matrix
\newcommand{\rowgrk}[1]{\boldsymbol{#1}} % for a row of a matrix, greek letters
\newcommand{\col}[1]{\mathbf{\MakeLowercase{#1}}} % for a column of a matrix
\newcommand{\colgrk}[1]{\boldsymbol{#1}} % for a column of a matrix, greek letters
\newcommand{\mat}[1]{\mathbf{\MakeUppercase{#1}}} % for a matrix
\newcommand{\matCoeff}[1]{\MakeLowercase{#1}} % for a coefficient in a matrix
\newcommand{\vecc}[1]{\mathbf{#1}} % for a vector
\newcommand{\sumVec}[1]{|#1|} % sum of entries in a tuple
\newcommand{\card}[1]{\mathrm{Card}(#1)}

%% --------------------------------------
%% ------------- MATRICES ---------------
%% --------------------------------------
\newcommand{\trsp}[1]{#1^\mathsf{T}} %transpose
\newcommand{\matrow}[2]{{#1}_{#2,*}} % \mathrm{row}(#1,#2)}
\newcommand{\matcol}[2]{{#1}_{*,#2}} % {\mathrm{col}(#1,#2)}
\newcommand{\diag}[1]{\,\mathrm{diag}(#1)} % diagonal matrix with diagonal entries #1
\newcommand{\idMat}[1][\rdim]{\mat{I}_{#1}} % identity matrix of size mxm
\newcommand{\any}{\ast} % to put a star (indicating some element in a matrix)
\newcommand{\anyMat}{\boldsymbol{\ast}}% to put a bold star (indicating some matrix in a block matrix)

%% --------------------------------------
%% -------- POLYNOMIAL MATRICES ---------
%% --------------------------------------
\newcommand{\rdeg}[2][]{\mathrm{rdeg}_{{#1}}(#2)} % shifted row degree
\newcommand{\cdeg}[2][]{\mathrm{cdeg}_{{#1}}(#2)} % shifted column degree
\newcommand{\leadingMat}[2][\unishift]{\mathrm{lm}_{#1}(#2)} % leading matrix of polynomial matrix, default shifts = 0 (uniform)
\newcommand{\shiftMat}[1]{\mat{\var}^{#1\,}} % shift matrix, diagonal of powers of X with exponents given by #1
\newcommand{\shiftSpace}[1][\rdim]{\ZZ^{#1}} % space for shifts: tuple of rdim integers
\newcommand{\unishift}{\mathbf{0}} % notation for uniform shifts ~ [0,..,0]
\newcommand{\amp}[1][\shifts]{\mathrm{amp}(#1)} % amplitude of shift
\newcommand{\shift}[2][s]{#1_{#2}} % shifts letter, default: s
\newcommand{\shifts}[1][s]{\mathbf{#1}} % shifts vector
\newcommand{\sshifts}[1][\shifts]{|#1|} % sum of entries in shifts vector

%% --------------------------------------
%% ----------- REDUCED FORMS ------------
%% --------------------------------------
\newcommand{\popov}{\mat{P}} % (shifted) Popov form
\newcommand{\hermite}{\mat{H}} % Hermite form
\newcommand{\smith}{\mat{S}} % Smith form
\newcommand{\reduced}{\mat{R}} % (shifted) reduced form


%%%SECTION introduction, notation, problem
\newcommand{\order}{\sigma} % interpolation order
\newcommand{\mulmat}[1]{\mat{M}_{#1}}
\newcommand{\minDeg}{\delta}
\newcommand{\minDegs}{\boldsymbol{\delta}}

%%% KERNEL BASIS
\newcommand{\sys}{\mat{F}} % input matrix for kernel basis (sys = system, because computing kernel is close to solving linear system)
\newcommand{\sysSpace}[1][1]{\polMatSpace[\rdim][#1]} % space for input matrix
\newcommand{\sol}{\row{p}} % one element (solution)
\newcommand{\solSpace}{\polMatSpace[1][\rdim]} % space for all solutions
\newcommand{\mkb}{\mat{P}} % kernel basis
\newcommand{\mkbSpace}{\polMatSpace[\rdim]} % space for kernel bases
\newcommand{\piv}{\pi} % (non-)pivot index
\newcommand{\modulus}[1][m]{\mathfrak{#1}}
\newcommand{\Modulus}{\mathfrak{M}}
\newcommand{\subVec}[3]{#1_{[#2:#3]}} % notation for subvector i:j
\newcommand{\subMat}[5]{#1_{[#2:#3,#4:#5]}} % notation for submatrix i:j,k:l


%% ------------------------------------------------
%% --- Debugging. Should be eventually removed. ---
%% ------------------------------------------------

\newcommand{\todo}[1]{\textcolor{red}{#1}} % TODO remove
\newcommand{\improve}[1]{\textcolor{blue}{#1}} % TODO remove
\title{M2 Internship Report: \\
\textsc{Solving determinantal systems using homotopy techniques}}
\author{\textsc{Vu} Thi Xuan\\
Symbolic Computation Group \\
David R. Cheriton School of Computer Science\\ University of Waterloo, Canada} 
\date{
Supervised by:\\
\vspace*{0.3cm}
Éric \textsc{Schost}\\
David R. Cheriton School of Computer Science\\ University of Waterloo, Canada\\
\vspace*{0.3cm}
and \\
\vspace*{0.3cm}
Mohab \textsc{Safey El Din}\\ 
Sorbonne Universités, UPMC Univ. Paris 6\\
CNRS, INRIA Paris Center, LIP6, PolSys Team, France\\
\vspace*{1cm}
February 1 -- June 16, 2017
}
\begin{document}
\maketitle
%\tableofcontents
\begin{abstract}
Solving determinantal polynomial systems, that is systems whose equations are obtained as minors of polynomial matrices, is a recurring question in domains such as optimization or real algebraic geometry. Results known as of now are not entirely satisfying; for instance, there is no known algorithm that would solve such systems with a complexity depending on the expected number of solutions. 

Homotopy continuation techniques rely on following a deformation between the system one has to solve and another system, which is called start system, with a similar combinatorial structure, but whose solutions are easy to describe. In the context of determinantal systems, we define the start system by designing a \emph{start matrix} in such a way that points at which the rank of the matrix decreases, are easy to identify. 

In this report, we study how the symbolic homotopy techniques can be used in order to solve determinantal polynomial systems. We give an algorithm to find all isolated solutions for the determinantal system which is obtained from all maximal minors. 
\end{abstract}
\newpage

\section{Introduction}
%\addtocontentsline{toc}{\protect\newpage}
%\addcontentsline{toc}{Section}{abstract}
\label{sec:intro}
\subsection{Problem and \todo{motivations}} In what follows, $\field$ is a field, $\mat{X} = (X_1, \ldots, X_n)$ is a set of $n$ variables and $\field[\mat{X}]$ is the multivariate polynomial ring with coefficients in $\field$. Let $F \in \field[\mat{X}]^{p \times q}$ be a polynomial matrix, without loss of generity, we assume that $p \leq q$. For several reasons, one is interested in computing the set of points at which the evaluation of the matrix has rank at most $p-1$, called \emph{maximal rank} problem. We restrict to the case $n = q-p+1$. Hereafter, given a polynomial matrix $F \in \field[\mat{X}]^{p \times q}$ and a point $\mathbf{x} \in \bar{\field}^n$, the notation $F({\mathbf{x}})$ means the evaluation of the matrix $F$ at the point $\mathbf{x}$. 

\begin{pbm}[Maximal rank problem] \label{problem} Given a field $\field$, a matrix $F \in \field[\mat{X}]^{p \times q}$ with $p \leq q$ and $n = q-p+1$, compute the set of isolated points at which the evaluation of $F$ has rank at most $p-1$, that is compute the set
\[S := \{\mathbf{x} \in \bar{\field}^n : \mathrm{rank}(F({\mathbf{x}})) \leq p - 1 \}.\]
\end{pbm}

A polynomial is called a $p \times p$ \emph{minor} of $F$ if it is the determinant of a $p \times p$ submatrix of $F$. To study Problem \ref{problem}, we consider the system of all $p \times p$ minors of $F$, which is called  the \emph{determinantal system} of $F$. Indeed, these $p \times p$ minors simultaneously vanish at all the points which we are interested in, and then give rise to a study of the \emph{determinantal ideal} which is generated by all $p \times p$ minors of $F$. Therefore, in order to find the set $S$, we give an algorithm to compute the isolated solutions (see \improve{Section \ref{sec:not}}) of the determinantal system of $F$. 
\subsection{Contributions, related works, and outline}
Henceforth, we use the notation below for the input matrix
\[ F = 
\left( \begin{matrix}
f_{1,1} & \cdots & f_{1,q}\\
\vdots & \ddots & \vdots \\
f_{p,1} & \cdots & f_{p,q}
\end{matrix} \right), \ \mathrm{where} \ f_{i,j} \in \field[\mat{X}] \ \mathrm{for} \ 1 \leq i \leq p, 1 \leq j \leq q.
\]
We will present it by means of a \emph{straight-line program}, that is, a sequence of elementary operations $+, -, \times$ that computes all of polynomials $f_{i,j}$ from the input variables $\mat{X}$. The \emph{length $\mathcal{E}$} of the input is the number of operations which we need to perform.

\paragraph{Contributions.} In order to solve Problem \ref{problem}, we study two cases of the input matrix. The first case, which is called \emph{column degrees}, is when the entries in the column $j$ have degrees at most $D_j$, that is, $\deg(f_{i,j}) \leq D_j$ for all $1 \leq i \leq p$; the second case (called \emph{row degrees}) the entries in the row $i$ have degrees at most $D_i$, that is, $\deg(f_{i,j}) \leq D_i$ for all $1 \leq j \leq q$. The elementary symmetric polynomials and the complete homogeneous symmetric polynomials (see \improve{Section \ref{sec:not}} for more details) play a significant role in our algorithm. Let $E_{q-p+1}(D_1, \ldots, D_q)$ and $S_{q-p+1}(D_1, \ldots, D_p)$ be the elementary symmetric polynomial of degree $q-p+1$ in $q$ variables $D_1, \ldots, D_q$ and the complete homogeneous symmetric polynomial of degree $q-p+1$ in $p$ variables $D_1, \ldots, D_p$, respectively. Under generic assumptions on the input, \cite[Proposition A.6~]{NieRan09} and \cite[Exercises 15.5 $\&$ 15.12]{Miller04} give us a bound for the the sum of the multiplicities of the isolated points of the corresponding determinantal ideal is either $E_{q-p+1}(D_1, \ldots, D_q)$ in row degrees case or $S_{q-p+1}(D_1, \ldots, D_p)$ in column degrees case. Meanwhile, we would like to give here a bound for that of any input matrix. 
\begin{Theorem}
The sum of the multiplicities of the isolated points of the corresponding determinantal ideal is at most $\mathcal{T}$, where $\mathcal{T}$ is either $E_{q-p+1}(D_1, \ldots, D_q)$ in the column degrees case, or $S_{q-p+1}(D_1, \ldots, D_p)$ in the row degrees case.
\end{Theorem}

For each case, we will construct an algorithm whose is in the polynomial of $\mathcal{T}, \mathcal{E}$ and the number of $p \times p$ minors of $F$. The output of the algorithm in each case is a zero-dimensional parametrization for the isolated solutions of the determinantal system of the input matrix $F$. Given a field $\field$, a set in $\bar{\field}^n$ is a \emph{zero-dimensional variety} if its cardinality is finite. Let $V \subset \bar{\field}^n$ be a zero-dimensional variety. A \emph{zero-dimensional parametrization} $\mathscr{R} = ((q,v_1, \ldots, v_n), \lambda)$ of $V$ consists in polynomials $(q,v_1, \ldots, v_n)$ such that $q \in \field[T]$ is monic and squarefree, all $v_i \in \field[T]$ and $\deg(v_i) < \deg(q)$, and $\lambda$ is a $\field$-linear form in $n$ variables, such that 
\begin{itemize}
\item $\lambda(v_1, \ldots, v_n) = Tq'$ mod $q$
\item we have $V = \{(\frac{v_1(\tau)}{q'(\tau)}, \ldots, \frac{v_n(\tau)}{q'(\tau)}) \ | \ q(\tau) = 0\}$;
\end{itemize}
the constraint on $\lambda$ says that the roots of $q$ are the values taken by $\lambda$ on $V$. The reasons for using the rational parametrization with $q'$ as the denominator is well-known \cite{Alonso1996, SaSc16, GiLeSa01, Rouillier1999} : when $\field = \mathbb{Q}$, it leads to a precise theoretical control on the size of the coefficients. 

The main idea behind our ingredient is to use the \emph{homotopy} 
\[H = (1-T)\cdot G + T\cdot F \in \field[T, \mat{X}]^{p \times q}\]
that connects a \emph{start matrix} $G$ to the target matrix $F$, where $T$ is a new variable. However, we need some properties on the start matrix. If we know the solutions for the determinantal system of $G$, by applying Newton iteration to the determinantal system of $H$, we can find a zero-dimensional parametrization for the isolated solutions for that of $F$. Hereafter, we use the notations $\f = (f_1, \ldots, f_M) \in \field[\mat{X}]^M$, $\g = (g_1, \ldots, g_M) \in \field[\mat{X}]^M$ and $\h = (h_1, \ldots, h_M) \in \field[T,\mat{X}]^M$ for the determinantal systems of input matrix $F$, start matrix $G$ and homotopy matrix $H$, respectively. The initial points for Newton iterations are solutions of $\g$. In order to use the Newton iterations for those points, we need a square subsystem of $\h$ corresponds to each point such that the Jacobian matrix of this system has full rank at the initial point. The Newton iterations stop at a suitable moment, for which we can deduce a zero-dimensional parametrization for the isolated set of $\f$.  Therefore, we need create a start matrix $G$ such that
\begin{itemize}
\item the solutions of its determinantal system can be found effectively,
\item we can extract a square subsystem with full-rank Jacobian,
\item there is no solution of its determinantal system is at infinity. \todo{Todo: Make more clear why}
\end{itemize}


\begin{Theorem} There is a randomized algorithm which solves Problem \ref{problem} in $\big({{q}\choose{p}} \,\mathcal{E}\,\mathcal{T}\big)^{\bigO{1}}$ operations in $\field$.
\end{Theorem}

%Previous works related to Problem \ref{problem} are given in \cite{FauSafSpa13, Miller04, Spa14, SaSc16, NieRan09}. I
\paragraph{Related works.} In \cite{FauSafSpa13}, the authors work on the case where the input matrix $F$ is a homogeneous polynomial matrix of degree $D$, that is $\deg(f_{i,j}) = D$ for all $1 \leq i \leq p, 1 \leq j \leq q$, under genericity assumptions on the input matrix. %By generic, they mean that there exists a non-identically null multivariate polynomial $h$ such that the complexity results hold when this polynomial does not vanish on the coefficients of the polynomials in the matrix \cite{FauSafSpa13}. 
The authors use Gröbner bases algorithms when the input is the deterninantal system of $F$. By using this algorithm, when the input matrix $F$ is homogeneous of degree $D$, Problem \ref{problem} can be solved in $\bigO{{q \choose p}{{Dm+1} \choose {q-p+1}}^{\expmatmul}}$ operations in $\field$, where $\expmatmul$ is the exponent of matrix multiplication, with the best known bound being $\expmatmul < 2.38$ \cite{CopWin90, LeGall14}. Notice that in \cite{FauSafSpa13}, the authors provide an algorithm for a more general problem: given a matrix $F$ whose entries are polynomials of degree $D$ in $\field[\mat{X}]$ and an integer $r < \min(p,q)$, compute the set of points at which the evaluation $F$ has rank at most $r$. By contrast, we work on general input matrix $F$ without generic assumptions when $r$ equals $p$. We remark that when the input matrix is homogeneous, we can use our algorithm in the case of column degrees.  In \cite{Spa14}, the author studies the determinantal ideal in the computing critical points. \todo{Todo: Make more precise}

Numerical homotopy techniques have paid a lot of attention in systems of polynomials; for instance, in~\cite{MORGAN872, MORGAN87, Morgan89,SAWC05,Zul88} and referred in their references. On symbolic side, in \cite{SaSc16}, the authors study the multi-homogeneous homotopy to compute a zero-dimensional parametrization of the algebraic set of a given system of polynomials. 

In determinantal problems, recent works on homotopy methods \cite{Ver89} show that they can be solved by numerical algorithms. However, we can not find any references, in our knowledge, for determinantal problems by using symbolic homotopy techniques. In this report, we study how symbolic homotopy techniques can be used to study the determinantal problems, in particular Problem \ref{problem}. 

\paragraph{Outline.} The report is organized as follows. We begin in Section~\ref{sec:not} with some notations as well as the
definitions of affine varieties and ideals over multivariate polynomial rings. Then, in Section~\ref{sec:startbound}, we give the construction of a start matrix in the cases of column degrees and row degrees. A bound on the number of isolated solutions of determinantal systems is also provided in Section~\ref{sec:bounddegree}. After that, in Section~\ref{sec:extractsubsystem}, we provide details how to extract a square subsystem for Newton iteration. Algorithms which are used to solve our problem are given in  Section~\ref{sec:Algorithms}. The report ends in Section~\ref{sec:conclusion} with conclusions and some perspectives for future research. 

\todo{Todo: Write more precise intro}
\section{Notations and preliminaries}
\label{sec:not}
Given a polynomial matrix $F \in \field[\mat{X}]^{p \times q}$, we write $F_{l:k \mathbf{;} e:f}$ for the submatrix of $F$ containing rows $l, \ldots, k$ and columns $e, \ldots, f$. We also write $F_{l:k \mathbf{;} *}$ for the submatrix of $F$ containing the rows $l, \ldots, k$ and all $q$ columns. The similar is applied for the matrix $F_{* \mathbf{;} e:f}$. 

We write $M_{pq}$ for the vector space of matrices with $p$ rows and $q$ columns over the field $\field$. Given a multivariate polynomial $g \in \field[\mat{X}]$, if we do not give a specific mention, we will consider the degree of $g$ is the total degree.  \todo{Todo: give a general reference} 

The elementary symmetric polynomial of degree $r$ in $m$ variables $t_1, \ldots, t_m$, written $E_{r}(t_1, \ldots, t_m)$, is defined as the coefficient of $x^r$ in $(1+t_1x)(1+t_2x)\cdots(1+t_mx)$. That is
\[E_{r}(t_1, \ldots, t_m) = \sum_{(i_1,\ldots,i_{m}) \subset \{1, \ldots, r\}^{m}}\prod_{j =1}^{m}D_{i_j}.\]

The complete homogeneous symmetric polynomial of degree $r$ in $m$ variables $t_1, \ldots, t_m$, written $S_{r}(t_1, \ldots, t_m)$, is defined as the coefficient of $x^{r}$ in $$\frac{1}{(1-t_1x)(1-t_2x)\cdots(1-t_mx)} = (1+t_1x + t_1^2x^2 + \cdots)\cdots(1+t_mx + t_m^2x^2 + \cdots).$$
That is 
\[
S_r(t_1, \ldots, t_m) = \sum_{i_1 + \cdots + i_m = r} t_1^{i_1}\ldots t_m^{i_m}. 
\]
There are some formulas which are obtained directly from the definition of the complete homogeneous symmetric polynomial, such as 
\[S_{r}(t_1, \ldots, t_m) =  \sum\limits_{k=1}^r\sum\limits_{(i_1, \ldots, i_k) \in \{1, \ldots, m\}^k}S_{r-k}(t_1, \ldots, t_k).\]
%\item[•] $S_{r}(t_1, \ldots, t_{m}) = \sum\limits_{i=0}^{r}t_1^{r-i}S_i(t_2, \ldots, t_m)$,
%$S_{q-m}(t_1, \ldots, t_{m+1}) = t_1^{q-m}+t_1^{q-m-1}S_1(t_2, \ldots, t_{m+1}) + t_1^{q-m-2}S_2(t_2, \ldots, t_{m+1}) + \cdots + S_{q-m}(t_2, \ldots, t_{m+1})$, that is, 
%\item[•] $S_r(t_1, \ldots, t_{m-1}, t_m) - S_r(t_1, \ldots, t_{m-1}, t_{m+1}) = (t_m - t_{m+1})S_{r-1}(t_1, \ldots, t_{m-1}, t_m, t_{m+1})$. 

\paragraph{Ideals and varieties.} Let $\field[\mat{X}]$ be a polynomial ring of $n$ variables. 
\begin{Definition} Let $I \subset \field[\mat{X}]$ be an ideal. The $\mathrm{radical}$ of $I$, denoted $\sqrt{I}$ is the set 
\[
\{f \ : \ f^m \in I \ \mathrm{for \ some} \ m \in \mathbb{N}, \ m \geq 1\}.
\]
\end{Definition}

An ideal $I \subset \field[\mat{X}]$ is \emph{prime} if whenever $f,g \in \field[\mat{X}]$ and $fg \in I$, then either $f \in I$ or $g \in I$. An ideal $I \subset \field[\mat{X}]$ is called a \emph{primary} ideal if $fg \in I$ implies $f \in I$ or $g^k \in I$ for some $k \in \mathbb{N}$. Remark that if $I$ is primary in $\field[\mat{X}]$, then $\sqrt{I}$ is a prime ideal. 
\begin{Definition}
Let $I$ be an ideal in $\field[\mat{X}]$. A \emph{primary decomposition} of $I$ is an expression 
\[
I = Q_1 \cap \cdots \cap Q_s,
\]
where each $Q_i$ is primary.
\end{Definition}
The decomposition is \emph{irredundant} or \emph{minimal} if $\sqrt{Q_i}$ are all distinct and $Q_i \not \supset \cap_{j \ne i}Q_j$ for all $1 \leq i \leq s$. After pruning redundant,  we obtain $P_i = \sqrt{Q}_i$ for $1 \leq i \leq r$. The radical ideals $P_i$ are then called the \emph{associated primes} of $I$. 
\begin{Theorem}$[\todo{cite}]$ Let $I \subset \field[\mat{X}]$ be an ideal. Then, there exist unique $\mathcal{P}_1, \ldots,, \mathcal{P}_r$ prime ideal in $\field[\mat{X}]$ such that 
\[
\sqrt{I} = \mathcal{P}_1 \cap \mathcal{P}_2 \cdots \cap \mathcal{P}_r
\]
and $\mathcal{P}_1, \ldots, \mathcal{P}_r$ are called $\mathrm{prime \ components \ of} \ \sqrt{I}$. 
\end{Theorem} 

\begin{Example}\label{Ex1} Let $\mathbb{C}[X,Y]$ be the polynomial ring of two variables with complex coefficients. Let $I = \langle X^2 - Y\rangle \subset \mathbb{C}[X,Y]$ be the ideal generated by $X^2 - Y$; and $J = \langle X^2, Y^3\rangle$ be the ideal generated by $X^2$ and $Y^3$. Then 
\begin{itemize}
\item $I$ is prime and $J$ is not prime;
\item the radical ideal of $I$ is $\sqrt{I} = \langle X^2 - Y\rangle$ and the radical ideal of $J$ is $\sqrt{J} = \langle X,Y \rangle$. 
\end{itemize}
\end{Example}
\begin{Example}
Let $I = \langle X^2, XY \rangle \subset \mathbb{C}[X,Y]$. A primary decomposition of $I$ is 
\[
I = \langle X^2, XY \rangle = \langle X \rangle \cap \langle X^2Y \rangle, 
\] where $P_1 = \langle X \rangle$ and $P_2 =  \langle X,Y \rangle$ are the associated primes of $I$. 
\end{Example}
\begin{Definition} $\mathrm{[Dimension \ of \ an \ ideal]}$ Let $\mathcal{P} \subset \field[\mat{X}]$ be a prime ideal. We say that d is the $\mathrm{Krull \ dimension }$ of $\mathcal{P}$ if and only if there exists prime ideals $\mathcal{P}_0, \mathcal{P}_1, \ldots, \mathcal{P}_d$ in $\field[\mat{X}]$ such that 
\[
\mathcal{P}_0 \subsetneq \mathcal{P}_1 \subsetneq \cdots \subsetneq \mathcal{P}_d = \mathcal{P}.
\] 
The $\mathrm{Krull \ dimension}$ of an ideal $I \subset \field[\mat{X}]$ is the maximum Krull dimension of the prime components of $\sqrt{I}$. 
\todo{Todo: check again}
\end{Definition} 
\begin{Example} Let $I$ be the ideal defined as in the Example \ref{Ex1}. Then, the dimension of $I$ equals one. 
\end{Example}
The geometric objects corresponding to ideals of $\field[\mat{X}]$ are \emph{affine \ varieties} (or \emph{algebraic sets}) of $\bar{\field}^n$. An affine variety in $\bar{\field}^n$ is the set of all solutions of a system of equations. Hereafter, we will use variety for the short of affine variety. Given $\f = (f_1, \ldots, f_M)$ in $\field[\mat{X}]^M$, we write $V(\f) \in \bar{\field}^n$ for the variety defined by $\f$. That is 
\[
V(\f) = \{{\bm \alpha} = (\alpha_1, \ldots, \alpha_n) \in \bar{\field}^n \ | \ f_i({\bm \alpha}) = 0 \ \mathrm{for \ all} \ 1 \leq i \leq M\}. 
\]

One of the important properties of varieties of $\bar{\field}^n$ is they define a topology on $\bar{\field}^n$, namely \emph{Zariski topology}, where the closed sets of the topology are the varieties. The \emph{Zariski closure} of a set $S$ in $\bar{\field}^n$ is the smallest (for the inclusion ordering) Zariski closed set that contains $S$. A set $V$ is \emph{Zariski dense} in a set $W$ if the Zariski closure of $V$ contains in the Zariski closure of $W$. The Zariski topology will be useful for defining an algebraic notion of genericity for structured system. 
\begin{Definition} A property of a family of systems $\mathfrak{F} \subset \bar{\field}[\mat{X}]$ which is a $\bar{\field}$-vector space of finite dimension is said to be $\mathrm{generic}$ if this property is satisfied on a nonempty open subset of $\mathfrak{F}$.
\end{Definition}
\begin{Example} \todo{Todo: give an example}
\end{Example}
\begin{Definition}$\mathrm{[Dimension \ of \ a \ variety]}$ Let $V$ be a variety. The dimension of V is the largest d such that the image of V by $(X_1, \ldots, X_n) \to (X_{i_1}, \ldots, X_{i_d})$ is Zariski dense for some $(i_1, \ldots, i_d) \subset \{1, \ldots, n\}$.
\end{Definition}
It is equivalent to define the dimension of a variety $V$ as the Krull dimension of $\field[\mat{X}]/I(V)$, where \[I(V) := \{f \in \field[\mat{X}] \ | \ f(\alpha) = 0 \ \forall \ \alpha \in V\},\]
 which is called the ideal of $V$. A zero-dimensional variety is a finite set. A variety $V \subset \bar{\field}^n$ is \emph{irreducible} if whenever $V$ is written in the form $V = V_1 \cup V_2$, where $V_1, V_2$ are varieties, then either $V = V_1$ or $V = V_2$. The next result is given in \cite[Theorem~4 -- section~6 -- chapter~4]{Cox07}.
\begin{Theorem} Let $V \subset \bar{\field}^{n}$ be a variety. Then V has a minimal decomposition 
\[
V = V_1 \cup \cdots \cup V_m,
\] where each $V_i$ is an irreducible variety and $V_i \not\subset V_j$ for $i \ne j$. Furthermore, this minimal decomposition is unique up to the order in which $V_1, \ldots, V_m$ are written; $V_1, \ldots, V_m$ are called the $\mathrm{irreducible \ components}$ of V.
\end{Theorem}

\begin{Definition} A variety is said to be $\mathrm{equidimensional}$ if and only if there exists $d \in \mathbb{N}$ such that all its irreducible components have dimension d. 
\end{Definition}

When $\field$ is algebraically closed, there is one-to-one correspondence between irreducible varieties in $\field^n$ and prime ideals in $\field[\mat{X}]$. 

\begin{Example}Let $f = X^2-Y \in \mathbb{C}[X,Y]$ and $g = X-Y$; then the variety $V(f) = \{(t^2,t) \ | \ t \in \mathbb{C}\}$ has dimension one while the variety $V(f,g) = \{(0,0), (1,-1)\}$ zero-dimensional. Moreover, $V(f)$ is irreducible (since $\langle f \rangle$ is prime) while $V(f,g)$ is not an irreducible variety (since $\langle f,g \rangle$ is not a prime ideal). 
\end{Example}

\begin{Example} Let $V$ be the variety which is defined by $X^2 - YZ = XZ-X = 0$ in $\mathbb{C}^3$, then $V = V_1 \cup V_2 \cup V_3$, where $V_1 = V(X,Y)$, $V_2 = V(X,Z)$ and $V_3 = V(Z-1,X^2 - Y)$. In other words, $V_1$ is the $Z$-axis, $V_2$ is the $Y$-axis and $V_3$ is a parabola.  Since $\langle X,Y\rangle$, $\langle X,Z\rangle$ and $\langle Z-1,X^2 - Y \rangle$ are prime ideals in $\mathbb{C}[X,Y,Z]$ then $V_1, V_2, V_3$ are irreducible. Therefore, $V_1, V_2, V_3$ are irreducible components of $V$. Moreover, all of these components has dimension one, so $V$ is equidimensional of dimension one. 
\end{Example}

A \emph{hyperplane} $H$ is the vanishing set of a linear polynomial in $\field[\mat{X}]$, that is $H = V(\sum a_iX_i + b)$. 
\begin{Definition}$\mathrm{[Degree \ of \ a \ variety]}$ Let V be an equidimensional variety of dimension d. The $\mathrm{degree}$ of $V$ is the unique integer $D$ such that $V \cap H_1 \cap \cdots \cap H_d$ consists of D points for generic hyperplanes $H_1, \ldots, H_p$. 

For any variety V, the degree of V is the sum of degrees of all irreducible components of V. \todo{Todo: Check the existence of $H_i$}
\end{Definition}

Given a set of polynomials $\mathbf{f} = (f_1, \ldots, f_M) \subset \field[\mat{X}]^M$, the \emph{Jacobian matrix} of $\mathbf{f}$ is an $M \times n$ matrix defined as follows 
\[
\mathrm{Jac}(\mathbf{f}) = \left[ \begin{matrix}
\frac{\partial f_1}{X_1}  & \cdots & \frac{\partial f_1}{X_n}\\
\vdots & \ddots & \vdots\\
\frac{\partial f_M}{X_1}  & \cdots & \frac{\partial f_M}{X_n}
\end{matrix} \right].
\]
%Jacobian criterion is an useful tool to check wheather the variety of an ideal is equidimensional. 
%\begin{Theorem}[Jacobian Criterion] $[\todo{cite}]$  \
%Let $\mathbf{f} = (F_1, ..., F_k) \subset \field[\mat{X}]^k$ and $V(\mathbf{f}) \subset \bar{\field}^n $. Assume %that $V(\mathbf{f})$ is nonempty and
%\[
%\{\mathbf{x} \in V(\mathbf{f}) \ | \ \mathrm{rank}(\mathrm{Jac}(\mathbf{f})({\mathbf{x}})) < k\} = \emptyset .
%\] Then, V is equidimensional and has dimension $n - k$.
%\end{Theorem}

\paragraph{Solutions of polynomial systems.} Given a polynomial system of $N$ equations in $n$ unknowns, $A(\mat{X}) = \mat{0}$, we are interested in $\mathbf{x}^*$, an \emph{isolated} solution of $A(\mat{X}) = \mat{0}$:
\[
\mathrm{for \ small \ enough} \ \epsilon > 0 : \{\mathbf{y} \in \bar{\field}^n : ||\mathbf{y} - \mathbf{x}^*|| < \epsilon\} \cap A^{-1}(\mat{0}) = \{\mathbf{x}^*\}, 
\] where $A^{-1}(\mat{0}) := \{\mathbf{y} \in \bar{\field}^n | A(\mathbf{y}) = \mat{0}\}$. 
\todo{Todo: Given the algebraic def NOT geometric def}

If ${\bf x}^*$ is isolated, we call $\mathbf{x}^*$ a \emph{singular} solution of $A(\mat{X}) = \mat{0}$ if and only if $\mathrm{rank}(\mathrm{Jac}(A)(\mathbf{x}^*)) < n$.  Let us denote $\mathrm{mult}(\mathbf{x}^*)$ for the multiplicity of the solution $\mathbf{x}^*$ of $A(\mat{X}) = \mat{0}$.   A solution $\mathbf{x}^*$ is called a \emph{simple} root of $A(\mat{X}) = \mat{0}$ if  $\mathrm{rank}(\mathrm{Jac}(A)(\mathbf{x}^*)) = n$.
%\paragraph{Zero-dimentional parametrization.} 

\section{Start matrix and a bound on the degree of the input}
\label{sec:startbound}
We study two cases for the input matrix of Problem~\ref{problem}, those are column degrees and row degrees. We recall here that the notation $\mathcal{T}$ is either $E_{q-p+1}(D_1, \ldots, D_q)$ in the case of column degrees or $S_{q-p+1}(D_1, \ldots, D_p)$ in the row degrees case. Before going in the details of the start matrix, we need to define some genericity assumptions for the case of row degrees. Hereafter, give a polymomial matrix $G$, we use the notation ${\bf g} = (g_1, \ldots, g_M)$ for the determinantal system of $G$. 
\subsection{Genericity assumptions}
\label{subsec:genericity}
Let $G = [g_{i,j}]_{1 \leq i \leq p, 1 \leq j \leq q}$ be a matrix where $g_{i,j}$ is a product of $D_i$ linear forms, with each linear form has generic coefficients. We recall here that $D_i$ is an integer that $\deg(f_{i,j}) \leq D_i$ for all $1 \leq j \leq q$. Notice that we use these genericity assumptions for only the row degrees case. 

We say that the matrix $G$ satisfies assumption $\mathcal{A}$ if 
\begin{enumerate}
\item rank($G(\mathbf{x}^*)$) $= p-1$ for all $\mathbf{x}^* \in V({\bf g})$.
\item ${\bf g}$ has exactly $S_{n}(D_1, \ldots, D_p)$ distinct solutions.  
\item ${\bf g}$ is a radical ideal (this genericity condition is equivalent to the property that $\mathrm{Jac}({\bf g})(\mathbf{x}^*)$ has full rank for any $\mathbf{x}^* \in V({\bf g}))$. 
\end{enumerate}

Let us write $\mathcal{G} = \{\gamma_{i,j}^{(t,l_i)} \ | \ 1 \leq j \leq q, 0 \leq t \leq n \ \mathrm{and} \ i : 1 \leq i \leq p, 1 \leq l_i \leq D_i\}$ for the set of new indeterminantes for these generic coefficients. For $1 \leq i \leq p, 1 \leq j \leq q$ write $\mathfrak{g}_{i,j} = \prod_{l=1}^{D_i}(\gamma_{i,j}^{(n,l_i)}X_n + \cdots + \gamma_{i,j}^{(1,l_i)}X_1 + \gamma_{i,j}^{(0,l_i)}) \in \field[\mathcal{G}, \bf{X}]$. We will prove that assumption $\mathcal{A}$ is generic in the following sense. 
\begin{Proposition} \label{generic}For any $k \in \{1,2,3\}$, there exists a nonempty Zariski open set $\mathcal{O}_k$ such that $\g$ satisfies $\mathcal{A}(k)$ for $\mathfrak{g}_{i,j} \in \mathcal{O}_k$. 
\end{Proposition}
\begin{proof}
Let us define the matrices $G_1$ and $G_2$ as follow
\[G_1 = \left( \begin{matrix}
\mathfrak{g}_{1,1} & \mathfrak{g}_{1,2} & \cdots  & \mathfrak{g}_{1, q}\\
\mathfrak{g}_{2,1} & \mathfrak{g}_{2,2} & \cdots  & \mathfrak{g}_{2, q}\\
\vdots & \vdots & \ddots & \vdots \\
\mathfrak{g}_{p,1} & \mathfrak{g}_{p,2} & \cdots  & \mathfrak{g}_{p, q}
\end{matrix} \right) \ \mathrm{and} \ 
 G_2 = \left( \begin{matrix}
\mathfrak{g}_{1,1} & 0 & \cdots & 0 & \mathfrak{g}_{1,p+1} & \cdots & \mathfrak{g}_{1, q}\\
0 & \mathfrak{g}_{2,2} & \cdots & 0 & \mathfrak{g}_{2,p+1} & \cdots & \mathfrak{g}_{2, q}\\
\vdots & \vdots & \ddots & \vdots & \vdots & \ddots & \vdots\\
0 & 0 & \cdots & \mathfrak{g}_{p,p} & \mathfrak{g}_{p,p+1} & \cdots & \mathfrak{g}_{p, q}
\end{matrix} \right). \] 
The idea to prove $\mathcal{A}(k)$, for any $k \in \{1,2,3\}$, is by using induction on the number of variables $n$ for the matrix of form $G_1$. Using the induction properties for smaller values of $n$, we show that property $\mathcal{A}(k)$ is true for the matrix $G_2$. Finally, by using this property of form $G_2$, we prove by a geometric argument that $\mathcal{A}(k)$ holds for any matrix of form $G_1$.

The complete proof is given in \improve{\cref{sec:proofgeneric}}. 
\end{proof}
\subsection{Start matrix}
In order to use the symbolic homotopy techniques, we need a start matrix $G$ that connects to the target matrix $F$ with some properties which we have mentioned in \improve{Section~\ref{sec:intro}}. 
\subsubsection{Column degrees}
\label{subsec:cd}
We first consider the case when $F = [f_{i,j}] \in \field[\mat{X}]^{p\times q}$ with $\deg({f_{i,j}}) \leq D_j$ for all $1 \leq i \leq p$. We will construct a polynomial matrix $G \in \field[\mat{X}]^{p \times q}$ such that the determinantal system of $G$ has $E_{n}(D_1, \ldots, D_q)$ solutions and we can find those solutions effectively. 

In this subsection, we work over a field $\field$ of characteristic zero. For any $1 \leq i \leq p, 1 \leq j \leq q$, let us define $\lambda_{i,j}^{(k)}  = (i+j)^k \in \field$ and $L_{i,j} = \sum_{k = 1}^{n}\lambda_{i,j}^{(k)}X_k + \lambda_{i,j}^{(0)}$. Then, we define the matrix $G$ as
\[G = 
\left( \begin{matrix}
g_1 & 2g_2 & \cdots & qg_{q}\\
g_1 & 2^2g_2 & \cdots & q^2g_q\\
\vdots & \vdots & \ddots & \vdots \\
g_1 & 2^pg_2 & \cdots & q^pg_q
\end{matrix} \right),
\]
where $g_{i}$ is the product of $D_i$ linear forms $g_i = \prod_{j = 1}^{D_i}L_{i,j}$. 
\begin{Lemma} \label{G} Let G be the matrix as we define above and $\g$ be the ideal generated by $p \times p$ minors of $G$. Then, $\g$ has exactly $E_{n}(D_1, \ldots, D_q)$ solutions and there is an algorithm, namely Algorithm \ref{StartMatCol}, to compute these solutions in $(nE_{n}(D_1, \ldots, D_q))^{\bigO{1}}$ operations in $\field$.
\end{Lemma}
\begin{proof}
Any $p \times p$ minor of $G$ has the form 
\[
f_G = \lambda g_{i_1}\ldots g_{i_p}, \ \mathrm{where} \ (i_1, \ldots, i_p) \in \{1, \ldots,q\}^{p} \ \mathrm{and} \ \lambda \in \field, \lambda \ne 0.
\] Then, for any solution $\mathbf{x} \in V({\bf g})$, $\mathbf{x}$ is a solution of $n$ polynomials which are taken from $\{g_j\}_{1 \leq j \leq q}$. This implies 
\begin{eqnarray*}
\#\{\mathrm{solutions \ of \ } {\bf g} \} &=& \sum_{(i_1, \ldots, i_{n}) \subset \{1, \ldots,q\}^{n}, \ i_j \ne i_k} \#
\{\mathrm{solutions \ of \ }  g_{i_1} = \cdots = g_{i_{n}} = 0  \} \\
&=& \sum_{(i_1, \ldots, i_{n}) \subset \{1, \ldots,q\}^{n}, \ i_j \ne i_k}\prod_{j =1}^{n}D_{i_j} = E_{n}(D_1, \ldots, D_q). 
\end{eqnarray*}

For the complexity, since each polynomial $L_{i,j}$ is linear, so the time to solve the system in the step $2.i$ is $n^{\bigO{1}}$; and there are $E_{n}(D_1, \ldots, D_p)$ systems which we need to solve. Therefore, the complexity of the Algorithm $\ref{StartMatCol}$ is $(nE_{n}(D_1, \ldots, D_q))^{\bigO{1}}$.

\end{proof}

\begin{algorithm}
\caption{$\mathsf{Start Matrix Column Degrees}$}
\label{StartMatCol}
{\bf Input}: a matrix $G \in \field[\mat{X}]^{p \times q}$ which is defined as above.\\
{\bf Output}: $E_{n}(D_1, \ldots, D_q)$ solutions of $\g$. 

\begin{enumerate}
\item $S \gets \emptyset$
\item for any $(i_1, \ldots, i_{n}) \in \{1, \ldots, q \}^{n}$: 
\begin{itemize}
\item for any $(j_1, \ldots, j_n) \in \{1, \ldots, D_{i_1}\} \times \cdots \times \{1, \ldots, D_{i_n}\} $: 
\begin{enumerate}
\item $\mathbf{x} \gets$ solve the linear system $L_{i_1,j_1} = \cdots = L_{i_n,j_n} = 0$
\item $S = S \cup \{\mathbf{x}\}$
\end{enumerate}
\end{itemize}
\item return $S$
\end{enumerate}
\end{algorithm}
%Furthermore, by the construction of $G$ as above, we can see that there are no solution of ${\bf g}$ at infinity. 
\subsubsection{Row degrees}
We here consider the case when $F = [f_{i,j}] \in \field[\mat{X}]^{p\times q}$ with $\deg({f_{i,j}}) \leq  D_i$ for all $1 \leq i \leq p$. We will construct a polynomial matrix $G \in \field[\mat{X}]^{p \times q}$ such that the determinantal system of $G$ has $S_{n}(D_1, \ldots, D_p)$ solutions. 

As in the proof of Proposition \ref{generic}, we can use a matrix $G$ as in the form of $G_2$ under some genericity assumptions. So, let us define a start matrix $G$ as follows 

\[ G = \left( \begin{matrix}
g_{1} & 0 & \cdots & 0 & g_{1,p+1} & \cdots & g_{1, q}\\
0 & g_{2} & \cdots & 0 & g_{2,p+1} & \cdots & g_{2, q}\\
\vdots & \vdots & \ddots & \vdots & \vdots & \ddots & \vdots\\
0 & 0 & \cdots & g_{p} & g_{p,p+1} & \cdots & g_{p, q}
\end{matrix} \right), \] where all of $\{g_i, g_{i,j}\}$ are a product of $D_i$ linear forms with generic coefficients. That is $g_i = \prod_{k=1}^{D_i}L_i^{(k)}$ and $g_{i,j} = \prod_{k=1}^{D_i}L_{i,j}^{(k)}$, where $L_i^{(k)}$ and $L_{i,j}^{(k)}$ are linear forms with generic coefficents. 

The next property helps us reduce our problem to the problem with smaller dimension. 
\begin{Proposition} \label{r2}
Let $\mathbf{x}$ be a solution of $\g$ and $k$ be an integer such that $k \leq \min(n,p)$. If $g_{i_1}(\mathbf{x}) = \cdots = g_{i_k}(\mathbf{x}) = 0$ and $g_{j}({\bf x}) \ne 0$ for $j \in \{1, \ldots, p\} \backslash \{i_1, \ldots, i_k\}$, then 
\[\mathrm{rank}(G_{i_1:i_k\mathbf{;}p+1:q}({\mathbf{x}})) \leq k-1, \]
where $G_{i_1:i_k\mathbf{;}p+1:q}$ is the submatrix of $G$ that contains rows $i_1, \ldots, i_k$ and columns $p+1, \ldots q$ of $G$. 
\end{Proposition}
\begin{proof} Without of loss of generality, we prove this result for $(i_1, \ldots, i_k) = (1, \ldots, k)$. This means we have $g_{1}(\mathbf{x}) = \cdots = g_{k}(\mathbf{x}) = 0$ and $g_{j}(\mathbf{x}) \ne 0$ for any $j \in \{k+1, \ldots, p\}$. Let $G_{1:p;*} \in \field[\mat{X}]^{p \times p}$ be the submatrix of $G$ consisting $p$ rows and the columns $k+1, \ldots, p, j_1, \ldots, j_k$ of $G$, where $p+1 \leq j_i \leq q$. Let $f_{1:p;*}$ is the determinant of $G_{1:p;*}$. Then, for any solution $\mathbf{x}$ of $\g$, $g_{1:p;*}(\mathbf{x}) = 0$. Moreover, $f_{1:p;*} = \det(G_{1:p;*}) = g_{k+1,k+1} \ldots g_{p,p} \det(G_{1:k;j_1:j_k})$. Therefore, $\det(G_{1:k;j_1:j_k})(\mathbf{x}) = 0$. This holds for any $(j_1, \ldots, j_k) \in \{p+1, \ldots, q\}^{k}$. This implies that $\mathrm{rank}(G_{i_1:i_k\mathbf{;}p+1:q}({\mathbf{x}})) \leq k-1$. 

\end{proof}

Therefore, in order to find the solutions for $\g$, we use the recursive algorithms for Problem~\ref{problem} with the inputs are of smaller dimension. In each recursive call, we use either algorithm $\mathsf{Row Determinantal System}$ or algorithm $\mathsf{ColumnDeterminantal System}$ which are given at the end of \improve{Section~\ref{sec:Algorithms}}. 
\begin{algorithm}[]
\caption{$\mathsf{Start Matrix Row Degrees}$}
\label{StartMatRow}
{\bf Input}: a matrix $G \in \field[\mat{X}]^{p \times q}$ which is defined as above.\\
{\bf Output}: $S_{n}(D_1, \ldots, D_p)$ solutions of ${\bf g}$.
\begin{enumerate}
\item $m = \min \{n,p\}$
%\item $S := \emptyset$
\item for $k = 1$ to $m$: 
\begin{enumerate}
\item for any  $(i_1, \ldots, i_k) \subset \{1, \ldots, p\}^k$:
\begin{itemize}
\item[\emph{i}.] from $g_{i_1} = \cdots = g_{i_k} = 0$, rewrite $\{X_i\}_{i = 1}^k$ in the form of $\{X_{i}\}_{i=k+1}^n$
\item[\emph{ii}.] substitute $\{X_i\}_{i = 1}^k$ into $G_{i_1:i_k\mathbf{;}p+1:q}$
\item[\emph{iii}.] $p \gets k; q \gets q-p; n \gets n - k$
\begin{itemize}
\item[•] if $p \leq q$:
\begin{itemize}
\item $S' \gets \mathsf{RowDeterminantal System}(G_{i_1:i_k\mathbf{;}p+1:q}, G_{i_1:i_k;i_1:i_k \cup i_{n}:i_{n+k-1}})$
\end{itemize}

\item[•] else:
\begin{itemize}
\item $S'\gets \mathsf{ColumnDeterminantal System}(G_{i_1:i_k\mathbf{;}p+1:q},\bar{G})$, where 
\[\bar{G} \in \field[X_{k+1}, \ldots, X_n]^{p \times q} \ \mathrm{is \ a \ start \ matrix \ for} \ G_{i_1:i_k\mathbf{;}p+1:q} \] 
\end{itemize}
\item[•] deduce a zero-dimensional parametrization $S$ for $V(\g)$ from $S'$
\end{itemize}
\item[\emph{iv}.] $S \gets S \cup \{\mathbf{x}\}$
\end{itemize}
\end{enumerate}
\end{enumerate} 
\end{algorithm}

\begin{Lemma}\label{P3} Let G be the matrix as we define above and $\g$ be the ideal generated by $p \times p$ minors of $G$. Then, $\g$ has exactly $S_{n}(D_1, \ldots, D_p)$ solutions. Moreover, there is an algorithm, namely Algorithm \ref{StartMatRow}, to compute these solutions in \todo{Complexity}. 
\end{Lemma}
\begin{proof}
The number of solutions for $\g$ is hold by using the genericity assumption $\mathcal{A}(2)$, so we need give here the algorithm complexity \todo{finish with complexity}.  
\end{proof}

\section{A bound for the number isolated solutions of determinantal systems}
\label{sec:bounddegree}
Let ${\bf g} = (g_1, \ldots, g_M)$ in $\field[\mat{X}]^M$ be the determinantal system of $G$, and let $\mathcal{T}$ be the number of solutions of ${\bf g}$. That is $\mathcal{T} = E_{n}(D_1, \ldots, D_q)$ in the column degrees case, and $\mathcal{T} = S_{n}(D_1, \ldots, D_p)$ in the row degrees case. We also write ${\bf f} = (f_1, \ldots, f_M)$ in $\field[\mat{X}]^M$ for the determinantal system of $F$. In this section, we are going to prove that the number of the isolated solutions of ${\bf f}$ is at most $\mathcal{T}$. 

\subsection{Some properties of a parametric system of equations}
Let $\mat{X}=(X_1,\dots,X_N)$ be indeterminates over $\field$, as above, and let $T$ be a new variable. We consider polynomials $\mathbf{h}=(h_1,\dots,h_M)$ in $\field[T,\mat{X}]$, with $M \ge n$, and the ideal $J=\langle \mathbf{h} \rangle \subset \bar{\field}[\mat{X}]$; for $\tau$ in $\bar{K}$, we write $\mathbf{h}_\tau=(h_{\tau,1},\dots,h_{\tau,M})=\mathbf{h}(\tau,\mat{X})\subset \bar{\field}[\mat{X}]$. %An ideal $I$ of a Noetherian ring $A$ is called \emph{unmixed} if the height of $I$ is equal to the height of every associated prime of $A/I$. The \emph{unmixedness theorem} is said to hold for the ring $A$ if every ideal $I$ generated by a number of elements equal to its height is unmixed. 
In this subsection, we give some general properties of the system $\mathbf{h}$, that hold under a few assumptions.  First, consider
the following properties related to the system $\mathbf{h}$ itself.
\begin{description}
\item[$\mathsf{H}_1.$] Any irreducible component of $V(J) \subset
  \bar{\field}{}^{n+1}$ has dimension at least one.
\item[$\mathsf{H}_2.$] For any prime $P \subset\bar{\field},\mat{X}]$, if the
  localization $J_P \subset \bar{\field}[T,\mat{X}]_P$ has height $n$, then it is
  unmixed (that is, all associated primes have height $n$).
%% \item[${\sf H}_3.$] There exists $\tau$ in $\KKbar$ that satisfies  ${\sf G}(\tau)$.
\end{description}

An obvious example where such properties hold is when $M=n$. Then, ${\sf H}_1$ is Krull's theorem, and ${\sf H}_2$ is Macaulay's unmixedness theorem in the Cohen-Macaulay ring $\bar{\field}[T,\mat{X}]_P$~\cite[Corollary~18.14]{Eisenbud95}. More generally, these properties hold when $\h$ is the sequence of $p \times p$ minors of a $p \times q$ matrix with entries in $\field[\todo{T},\mat{X}]$, with $n=q-p+1$ (the case $M=n$ corresponds to $p=1$); see~\cite[Section~6]{Eagon188}.

Then, for $\tau$ in $\bar{K}$, we denote by $\mathsf{G}(\tau)$ the following three properties.
\begin{description}
\item[$\mathsf{G}_1(\tau).$] The ideal $\langle \mathbf{h}_\tau \rangle$ is radical in $\field[\mat{X}]$.
\item[$\mathsf{G}_2(\tau).$] For $k=1,\dots,M$,
  $\deg_\mat{X}(h_k)=\deg_\mat{X}(h_{\tau,k})$.
\item[$\mathsf{G}_3(\tau).$] The only common solution to $h_{\tau,1}^H(0,\mat{X})=\cdots=h_{\tau,M}^H(0,\mat{X})=0$ is $(0,\dots,0)\in \bar{\field}{}^n$, where for $k=1,\dots,M$, $h_{\tau,k}^H$ is the polynomial in $\bar{\field}[X_0,\mat{X}]$ obtained by homogenizing $h_{\tau,k}$ using a new variable $X_0$. In particular, $V(\mathbf{h}\tau) \subset \field{}^n$ is finite.
\end{description}

The main result in this subsection is the following.
\begin{Proposition}\label{degree_fiber}
Suppose that $\mathsf{H}_1$ and $\mathsf{H}_2$ hold. Then, there exists an integer $c$ such that for all $\tau$ in $\bar{\field}$, the sum of the multiplicities of the isolated solutions of $\mathbf{h}_\tau$ is at most $c$, and is equal to $c$ if $\mathsf{G}(\tau)$ holds.
\end{Proposition}
\begin{proof}
The proof is given in \improve{\cref{sec:proofdegree}}. 
\end{proof}
%Consider an irredundant primary decomposition of $J$ in $\bar{\field}[T, \mat{X}]$, of the form $J=Q_1 \cap \cdots \cap Q_r$, and let $P_1,\dots,P_r$ be the associated primes, that is, the respective radicals of $Q_1,\dots,Q_r$. We assume that $P_1,\dots,P_s$ are the minimal primes, for some $s \le r$, so that $V(P_1), \dots,V(P_s)$ are the irreducible components of $V(J)\subset \bar{\field}{}^{n+1}$. By ${\sf H}_1$, these irreducible components all have dimension at least one. Refining further, we assume that $t \le s$ is such that $V(P_1),\dots,V(P_t)$ are the irreducible components of $V(J)$ of dimension one whose image by $\pi_T: (\tau,x_1,\dots,x_n) \mapsto \tau$ is dense in $\field$. Let us write $J=J' \cap J''$, with $J'=Q_1 \cap \cdots \cap Q_t$ and $J''=Q_{t+1} \cap \cdots \cap Q_r$. We also introduce  $\frak{J}$, the extension of $J$ in $\bar{\field}(T)[\mat{X}]$, and similarly $\frak{J}'$ and ${\frak J}''$. In the proof of Proposition \ref{degree_fiber} which is given in \improve{\cref{sec:proofdegree}}, we can see $c=\dim_{\bar{\field}(T)}(\bar{\field}(T)[\mat{X}]/{\frak J}')$.
\subsection{Number of isolated solutions of $\f$}
Given matrices $M$ and $N$ in $\field[\mat{X}]^{n \times n}$, we define a matrix $Q$ in $\field[T, \mat{X}]^{n \times n}$ as $(1-T)\cdot M + T\cdot N$. The next proposition tells us the relations between their determinants. 
\begin{Proposition}\label{det} Let $M$, $N$ and $Q$ be the matrices we define above. Then,
\[
\det(Q) = (1-T)^n \det(M) + T\cdot f(T,\mat{X}) \ \mathrm{and} \ \det(Q) = (1-T)\cdot g(T,\mat{X}) + T^n \det(N),
\] where $h(T,\mat{X})$ and $g(T,\mat{X})$ are polynomials in $\field[T,\mat{X}]$.
\end{Proposition}
\begin{proof}
Let us write $M = [g_{i,j}]_{1 \leq i,j \leq n}, N = [f_{i,j}]_{1 \leq i,j \leq n}$ and $Q = [h_{i,j}]_{1 \leq i,j \leq n}$; then for any $i,j$,  $h_{i,j} = (1-T)g_{i,j} + Tf_{i,j}$. Then,
\begin{eqnarray*}
\det(Q) &=& \sum_{\sigma \in S_n}\mathrm{sgn}(\sigma)\prod_{i = 1}^nh_{i,\sigma_i}\\
&=& \sum_{\sigma \in S_n}\mathrm{sgn}(\sigma)\prod_{i = 1}^n\left[(1-T)g_{i,\sigma_i} + Tf_{i,\sigma_i}\right]\\
&=& \sum_{\sigma \in S_n}\mathrm{sgn}(\sigma)\left[(1-T)^n\prod_{i = 1}^ng_{i,\sigma_i} + \bar{f}(T,\mat{X})\right],
\end{eqnarray*}
where $\bar{h}(T,\mat{X}) = \prod_{i = 1}^n\left[(1-T)g_{i,\sigma_i} + Tf_{i,\sigma_i}\right] - (1-T)^n\prod_{i = 1}^ng_{i,\sigma_i}$. Moreover, $\bar{f}(T,\mat{X})$ is a polynomial in $\field[T, \mat{X}]$ with all the terms are multiple of $T$. Therefore, 
\begin{eqnarray*}
\det(Q) &=&  \sum_{\sigma \in S_n}\mathrm{sgn}(\sigma)\left[(1-T)^n\prod_{i = 1}^ng_{i,\sigma_i}\right] + \sum_{\sigma \in S_n}\mathrm{sgn}(\sigma)\bar{f}(T,\mat{X}) \\
&=& (1-T)^n\det(M) + T \cdot f(T,\mat{X})
\end{eqnarray*}
Similarly, we obtain $\det(Q) = (1-T)\cdot g(T,\mat{X}) + T^n\det(N)$.

\end{proof}

Let $\f = (f_1, \ldots, f_M), \g = (g_1, \ldots, g_M)$ and $\h = (h_1, \ldots, h_M)$ be the determinantal systems of $F$, $G$ and $H$, respectively. From Proposition $\ref{det}$, for any $h_i$, there exist $\bar{f}_i$ and $\bar{g}_i$ in $\field[T, \mat{X}]$ such that 
\begin{equation} \label{deteq}
h_i = (1-T)^pg_i + T.\bar{f}_{i} \ \mathrm{and} \ h_i = (1-T)\bar{g}_i + T^pf_i,
\end{equation}
which implies that ${\bf{f}} = {\bf{h}}(1, \mat{X}) = (h_{1,i})_{1 \leq i \leq M}$ and ${\bf{g}} = {\bf{h}}(0, \mat{X}) = (h_{0,i})_{1 \leq i \leq M}$.% Let $(g_{i}^H)_{1 \leq i \leq M}$ be the polynomials in $\bar{\field}[X_0, \mat{X}]^M$ obtained by homozenizing  $(g_i)_{1 \leq i \leq M}$ using a new variable $X_0$. 
%\begin{Proposition} The only common solution to $g_1^H(0,\mat{X}) = \cdots = g_M^H(0,\mat{X})$ is $(0, \ldots, 0)\in \bar{\field}^n$. 
%\end{Proposition}

Puiseux series are a generalization of power series that allow for negative and fractional exponents of the indeterminate $T$. Formally, given a field $\field$, the field of \emph{Puiseux series} with coefficients in $\field$ is the set of expressions of the form $f = \sum_{k \ge k_0}c_kT^{k/n}$. The \emph{valuation} $\nu(f)$ of a series $f = \sum_{k \ge k_0}c_kT^{k/n}$ is the smallest rational $k/n$ such that the coefficient $c_k$ of the term with exponent $k/n$ is non-zero.% Let $(h_{0,k}^H)_{1 \leq k \leq M}$ be the polynomials in $\bar{\field}[X_0, \mat{X}]^M$ obtained by homozenizing  $(h_{0,k})_{1 \leq k \leq M}$ using a new variable $X_0$. 

Let $(T, f_1(T), \ldots, f_n(T)) \in V(\h)$. The next proposition says that there is no solution of ${\bf g}$ at infinity. 
\begin{Proposition}\label{valuation}
For any $i \in \{1, \ldots, n\}$, $f_i(T)$ has non-negative valuation. 
\end{Proposition}
\begin{proof}
Without loss of generality, we can write $f_i(T) = \frac{s_i(T)}{T^d}$ with $\nu(s_i(T)) \ge 0$ for all $1 \leq i \leq n$. We will prove $d \leq 0$. Indeed, assume by contradiction that $d > 0$. From Proposition $\ref{det}$, for any $f_H \in \langle {\bf h} \rangle$, 
\begin{equation}\label{e1}
f_H = (1-T)^p f_G + T\cdot h(T, \mat{X}),
\end{equation}
 where $f_G \in \langle {\bf g} \rangle$ and $h(T, \mat{X}) \in \field[T, \mat{X}]$. Let $D$ be the total degree of $f_G$ and $f_G^{\mathrm{H}} \in \field[X_0, \mat{X}]$ be the homogenized polynomial of $f_G$ using a new variable $X_0$, that is, $f_G^{\mathrm{H}}(X_0, \mat{X}) = X_0^Df_G(\mat{X}/X_0)$. Then, 
 \begin{equation}\label{e2}
 f_G(\mathbf{x}(T)) = T^{-Dd}f_G^{\mathrm{H}}(T^d, s_1(T),\ldots, s_n(T)) 
 \end{equation}
From \cref{e1} and \cref{e2}, we have 
\[
(1-T)^pf_G^{\mathrm{H}}(T^d, s_1(T), \ldots, s_n(T))  + T.h(T, f_1(T), \ldots, f_n(T)).T^{Dd} = 0.
\]
On one hand, since the total degree of $h_{\mat{X}}(T, \mat{X})$ is at most $D$, then \[\nu(h(T, f_1(T), \ldots, f_n(T))T^{Dd}) \ge 0\] This means $\nu(T.h(T, f_1(T), \ldots, f_n(T)).T^{Dd}) > 0$. On the other hand, we claim that \[\nu(f_G^{\mathrm{H}}(T^d, s_1(T), \ldots, s_n(T))) = 0.\] Indeed, it is easy to see that $\nu(f_G^{\mathrm{H}}(T^d, s_1(T), \ldots, s_n(T))) \ge 0$. Furthermore, we claim that $f_G^{\mathrm{H}}(0, s_1(0), \ldots, s_n(0)) \ne 0$. Then, as a consequence,  
\[
(1-T)^pf_G^{\mathrm{H}}(T^d, s_1(T), \ldots, s_n(T))  + T.h(T, f_1(T), \ldots, f_n(T))T^{Dd} \ne 0
\]
 which is a contradiction. Thus, we need to prove that $f_G^{\mathrm{H}}(0, s_1(0), \ldots, s_n(0))$ is non-zero. Assume by contradiction that $f_G^{\mathrm{H}}(0, s_1(0), \ldots, s_n(0)) = 0$.
%\begin{enumerate}
\paragraph{Column degrees cases:} Since any $f_G \in \langle {\bf g} \rangle$, $f_G$ has the form $f_G = \lambda. g_{i_1} \ldots g_{i_p}$ where $(i_1, \ldots, i_p)\subset \{1, \ldots, q\}^p$ and $\lambda \in \field$, then 
\[
f_G^{\mathrm{H}}(X_0, \mat{X}) = \lambda.g_{i_1}^{\mathrm{H}}(X_0,\mat{X}) \ldots g_{i_p}^{\mathrm{H}}(X_0, \mat{X}).
\] This implies 
\[
f_G^{\mathrm{H}}(0, s_1(0), \ldots, s_n(0)) = \lambda.g_{i_1}^{\mathrm{H}}(0, s_1(0), \ldots, s_n(0)) \ldots g_{i_p}^{\mathrm{H}}(0, s_1(0),\ldots, s_n(0)),
\] i.e., 
\[
g_{i_1}^{\mathrm{H}}(0, s_1(0), \ldots, s_n(0)) \ldots g_{i_p}^{\mathrm{H}}(0, s_1(0), \ldots, s_n(0)) = 0.
\]
Moreover, $(0, s_1(0), \ldots, s_n(0))$ is a solution of $n$ polynomial equations $f_G^{\mathrm{H}}(T, \mat{X}) = 0$; and $g_{i_j}$ is defined as a product of linear forms as above. Therefore, 
\[
\left( \begin{matrix}
\beta_{1,1} & \cdots & \beta_{1,n}\\
\vdots &  \ddots & \vdots \\
\beta_{n,1} & \cdots & \beta_{n,n}\\
\end{matrix} \right)\left(\begin{matrix}
s_1(0)\\
\vdots \\
s_n(0)
\end{matrix}\right) = 0,
\]
where $(\beta_{i,1}, \ldots, \beta_{i,n}) \in \{(\lambda_{i,1}^{(1)}, \lambda_{i,1}^{(2)}, \ldots, \lambda_{i,1}^{(n)}), \ldots, (\lambda_{i,D_1}^{(1)}, \lambda_{i,D_1}^{(2)}, \ldots, \lambda_{i,D_1}^{(n)})\}$. By the construction of $G$ as above, we have the matrix $B \in \field^{n \times n}$ is full rank, where $B = [\beta_{i,j}]_{1 \leq i \leq n, 1 \leq j \leq n}$. From which and $B[s_1(0), \ldots, s_n(0)]^T = 0$ we deduce that $(s_1(0), \ldots, s_n(0)) = (0, ..., 0)$. This contradicts with the fact that $(s_1(0), \ldots, s_n(0)) \ne (0, \ldots, 0)$. 

\paragraph{Row degrees case:} In this case, each entry in row $i$ of the start matrix is a product of $D_i$ linear form with generic coefficients. Without loss of generality, let us assume that $L_1, \ldots, L_k$ receive $(s_1(0), \ldots, s_n(0))$ as a solutions, where $L_i$ is a linear form of $g_i$. We denote $\s = (s_1(0), \ldots, s_n(0))$. From Proposition~\ref{r2}, any $k \times  k$ minors of $G_{1:k;p+1:q}$ receives $\s$ as a solutions. We will see in \improve{\cref{sec:extractsubsystem}} that we can find $p-k$ equations from all $k \times  k$ minors of $G_{1:k;p+1:q}$ such that those equations together with $L_1, \ldots, L_k$ build a square subsystem $n$ equations that receives $\s$ as solution. Let us denote those $p-k$ equations as $L_{k+1}, \ldots, L_n$. Let $L_i^{\mathrm{H}}(X_0, \mat{X})$ be the homogenize polynomial of $L_i(\mat{X})$. \todo{Todo: To finish}

%Hence, we obtain $f_G^{\mathrm{H}}(0, s_1(0),\ldots, s_n(0)) \ne 0$. Therefore, $\nu(f_G^{\mathrm{H}}(T^d, s_1(T), \ldots, s_n(T))) = 0$, this implies $\nu((1-t)^pf_G^{\mathrm{H}}(T^d, s_1(T), \ldots, s_n(T))) = 0$. Thus, \[(1-T)^pf_G^{\mathrm{H}}(T^d, s_1(T), \ldots, s_n(T))  + T.h(T, f_1(T), \ldots, f_n(T))T^{Dd} \neq 0\] which is a contradiction. 
\end{proof}
As we discuss in the previous subsection, the system $\h$ satisfies $\mathsf{H}_1$ and $\mathsf{H}_2$. Then from Proposition~\ref{degree_fiber}, there exists an integer $c$ such that for all $\tau$ in $\bar{\field}$, the sum of the multiplicities of the isolated solutions of $\mathbf{h}_\tau$ is at most $c$.  Moreover, since $\g = {\bf{h}}_0 = (h_i(0,\mat{X}))_{1 \leq i \leq M}$ and by using the genericity assumption $\mathcal{A}(3)$ in \improve{Section~\ref{subsec:genericity}}, we have $\langle {\bf{h}}_0 \rangle = \langle \g \rangle$ is radical in $\bar{\field}[\mat{X}]$. The property ${\sf G}_2(0)$ can be verified from the construction of $H$; and ${\sf G}_3(0)$ is an other expression of Proposition~\ref{valuation}. In other words, the assumption ${\sf G}(0)$ hold when $\h$ is the determinantal system of $H$. Therefore, the number of isolated solutions of $\mathbf{h}_0$ is exactly $c$ which equals $\mathcal{T}$. We apply again now Proposition~\ref{degree_fiber} for $\tau = 1$ to obtain the following result. 
\begin{Theorem}
Let $F \in \field[\mat{X}]^{p \times q}$ with $p \leq q$ and $\f$ be the ideal generated by the $p \times p$ minors of $F$. Then, the number of isolated solutions of $\f$ is at most $\mathcal{T}$. 
\end{Theorem}
\section{Excerpting square system}
\label{sec:extractsubsystem}
Given $\mathbf{x}^*$ in $V(\g)$, the aim of this section is extracting a square subsystem of determinantal system with full rank Jacobian for Newton iteration. 

Let $\mathbf{x}^*$ be a solution of $\g$ and $\bar{G} \in \field[\mat{X}]^{(p-1) \times (p-1)}$ be a submatrix of $G$ such that $\det(\bar{G})(\mathbf{x}^*) \ne 0$. Let $\bar{F}$ and $\bar{H}$ be the $(p-1) \times (p-1)$ submatrices of $F$ and $H$, respectively with the column indices as the $\bar{G}$ column indices. Then, $\bar{H} = (1-T).\bar{G} + T.\bar{F}$. By using the Proposition $\ref{det}$, we have 
\[
\det(\bar{H}) = (1-T)^{p-1} \det(\bar{G}) + T.h(T,\mat{X}),
\] where $h(T,\mat{X}) \in \field[T,\mat{X}]$. This implies 
\[
\det(\bar{H})(\mathbf{x}^*) = (1-T)^{p-1} \det(\bar{G})(\mathbf{x}^*) + T.h(T,\mat{X})(\mathbf{x}^*). 
\] From which and $\det(\bar{G})(\mathbf{x}^*) \ne 0$, we can deduce $\det(\bar{H})(\mathbf{x}^*)$ has zero-valuation. This means $\det(\bar{H})(\mathbf{x}^*) \ne 0$. We denote $m$ for $\det(\bar{H})$, thanks to $m$ we can construct the system of $n$ equations (see \improve{\cref{subsec:local}}). 

The structure of this section as follows. In \improve{\cref{subsec:local}}, we will present how to obtain this system if we know $m$; after that, in \improve{\cref{subsec:cd2}} and \improve{\cref{subsec:row2}}, we will show how to find the matrix $\bar{G}$ such that $\bar{G}(\mathbf{x}^*) \ne 0$ for fix $\mathbf{x}^* \in V(\g)$ in the case when we work on column degrees and row degrees respectively. As a consequence, we can build the matrix $\bar{H}$. 
\subsection{Local description}
\label{subsec:local}
The main goal of this section is giving a local description of the variety of $\h$. We will show that it sufficies to use $n$ equations to describe the local variety $V(\h)$. We follow the similar way as in \cite[Section ~ 2.2]{Bank2001}. 

Let $H = [h_{i,j}]$ be a $p \times q$ polynomial matrix of $n+1$ variables with $p \leq q$. Let $l$ and $k$ be any natural numbers with $l \leq q$ and $k \leq \min\{p,l\}$. Let $I_k = (i_1, \ldots, i_k) \subset \{1, \ldots, l\}^k$ be an ordered sequence and $M(I_k)$ be the determinant of $H_{1:k;I_k}$. The Exchange Lemma below will be an important tool in this section. 
\begin{Lemma}$\cite{Bank2001}$ Let $H$, $l$ and $k$ be given as above. Let $I_k = (i_1, \ldots, i_k)$ amd $I_{k-1} = (j_1, \ldots, j_{k-1})$ be two index sets such that $I_k \cap I_{k-1} \ne \emptyset$. Then, 
\[
M({I_{k-1}})M(I_k) = \sum_{j \in I_k \backslash I_{k-1}} \epsilon_j \ M(I_k \backslash \{j\}) \ M(I_{k-1} \cup \{j\}), 
\] for suitable number $\epsilon_j \in \{1, -1\}$. \label{exchange}
\end{Lemma}
The following proposition is a similar version of \cite[Proposition ~ 5]{Bank2001}. In \cite{Bank2001}, the authors use the matrix $H$ as the Jacobian matrix of $p$ polynomials $(f_1, \ldots, f_p) \in \field[\mat{X}]^p$ while here, we rewrite this proposition for the general polynomial matrix $H \in \field[T,\mat{X}]^{p \times q}$ where $p \leq q$. Noting that in \cite{Bank2001}, there is a condition that $p \leq n$. 

Let $m \in \field[T,\mat{X}]$ be the $(p-1) \times (p-1)$ minor of $H$ given by the first $(p-1)$ rows and $(p-1)$ columns, i.e., $m = \det(H_{1:p-1;1:p-1})$. Let us define $V(m) := \{\mathbf{x} \in \bar{\field[T]}^n : m(\mathbf{x}) = 0\}$ and $V(\h)_m : = V(\h) \backslash V(m)$, where $V(\h) = \{\mathbf{x} \in \bar{\field}^{n} : f_H(\mathbf{x}) = 0 \ \mathrm{for \ all} \ f_H \in \langle \h \rangle \}$. Hereafter, for any $1 \leq i_1 \leq \cdots \leq i_p \leq n$, let us denote $M(i_1, \ldots, i_p) \in \field[T,\mat{X}]$ for the determinant of the submatrix of $H$ which contains $p$ rows and the columns $i_1, \ldots, i_p$. 
\begin{Proposition} \label{ppp} Let $m, V(\h), V(\h)_m$ and $V(m)$ be defined as above. Then, 
\[
V(\h)_m = \{\mathbf{x} \in \bar{\field[T]}^n \ | \ M(1, \ldots, p-1, s) = 0, m(\mathbf{x}) \ne 0 \ \mathrm{for} \ s \in \{p, \ldots, q\} \}.
\] In other words, the variety $V(\h)$ is locally described by $q - p + 1$ polynomials $(outside \ of \ V(m))$, i.e., $n$ polynomials 
\[
M(1, \ldots, p-1, p), M(1, \ldots, p-1, p+	1), \ldots, M(1, \ldots, p-1, q).
\]
\end{Proposition}
\begin{proof}
$(\subseteq)$ It is obvious that for any $\mathbf{x} \in \bar{\field[T]}^n$ such that $\mathbf{x} \in V(\h)_m$ we have 
\[\mathbf{x} \in \{\mathbf{x} \in \bar{\field[T]}^n \ | \ M(1, \ldots, p-1, s) = 0, m(\mathbf{x}) \ne 0 \ \mathrm{for} \ s \in \{p, \ldots, q\} \}.\]

$(\supseteq)$ Let $\mathbf{x}$ be any point in $\bar{\field[T]}^n$ such that $m(\mathbf{x}) \ne 0$ and $M(1, \ldots, p-1, s)(\mathbf{x}) = 0$ for any $s \in \{p, \ldots, q\}$. We have to prove that $M(i_1, \ldots, i_p)(\mathbf{x}) = 0$ for any $(i_1, \ldots, i_p) \subset \{1, \ldots, q\}^p$. By using Lemma $\ref{exchange}$ for $M(1, \ldots, p-1) = m$, we have 
\[
m.M(i_1, \ldots, i_p) = \sum_{\{j \in \{i_1, \ldots, i_p\} \backslash \{1, \ldots, p-1 \} } \epsilon_j \ M(\{i_1, \ldots, i_p\} \backslash \{j\}) \ M(1, \ldots, p-1,j),
\] for suitable $\epsilon_j \in \{1,-1\}$. Therefore, 
\[
m(\mathbf{x}).M(i_1, \ldots, i_p)(\mathbf{x}) = \sum_{j \in \{i_1, \ldots, i_p\} \backslash \{1, \ldots, p-1 \} } \epsilon_j \ M(\{i_1, \ldots, i_p\} \backslash \{j\})(\mathbf{x}) \ M(1, \ldots, p-1,j)(\mathbf{x}). 
\] Moreover, $M(1, \ldots, p-1, s)(\mathbf{x}) = 0$ for any $s \in \{p, \ldots, q\}$ and $m(\mathbf{x}) \ne 0$, so $M(i_1, \ldots, i_p)(\mathbf{x}) = 0$ for any $(i_1, \ldots, i_p) \subset \{1, \ldots, q\}^p$. 

\end{proof}
\begin{Remark}The result in Proposition $\ref{ppp}$ remains true if we replace $m$ by any $(p-1) \times (p-1)$ minor of $H$. 
\end{Remark}
\subsection{Column degrees}
\label{subsec:cd2}
In this section, we will find a $(p-1)\times (p-1)$ submatrix of $H$, namely $\bar{H}$, such that $m(\mathbf{x}^*) \ne 0$, where $m = \det(\bar{H})$ and $\mathbf{x}^*$ is a solution of $\g$ when we work on the case $\deg(f_{i,j}) = D_j$ for all $1 \leq i \leq p$. Let us recall the starting matrix $G$ in this case is 
\[G = 
\left( \begin{matrix}
g_1 & 2g_2 & \cdots & qg_{q}\\
g_1 & 2^2g_2 & \cdots & q^2g_q\\
\vdots & \vdots & \ddots & \vdots \\
g_1 & 2^pg_2 & \cdots & q^pg_q
\end{matrix} \right),
\]
where $g_{i}$ is the product of $D_i$ linear forms, i.e., $g_i = \prod_{j = 1}^{D_i}L_{i,j}$. Moreover, any $p \times p$ minor of $G$ has the form $\lambda g_{i_1}\ldots g_{i_p}$, where $(i_1, \ldots, i_p) \in \{1, \ldots,q\}^{p}$ and $\lambda \in \field$.

Without loss of generity, we take $\mathbf{x}^*$ is a solution of the system of $q-p+1$ equations, i.e., $n$ equations $g_1 = \cdots = g_{q-p+1} = 0$. Let $\bar{G} \in \field[\mat{X}]^{(p-1) \times (p-1)}$ be a submatrix of $G_{*;q-p+2:q}$, where $G_{*;q-p+2:q} \in \field[\mat{X}]^{p \times (p-1)}$ contains the columns $q-p+2, \ldots, q$ of $G$. So, $\det(\bar{G}) = \lambda g_{q-p+2}\ldots g_{q}$ for $\lambda \in \field$. By the construction of $\{g_i\}_{i=1}^{q}$, we have $g_i(\mathbf{x}^*) \ne 0$ for all $q-p+2 \leq i \leq q$. As a consequence, $\det(\bar{G})(\mathbf{x}^*) \ne 0$. 

Furthermore, by the construction of $g_{i}$, it can be verified that the Jacobian matrix of this system at $\mathbf{x}^*$ has full rank.
\subsection{Row degrees}
\label{subsec:row2}
In this section, we will find a $(p-1)\times (p-1)$ submatrix of $H$, namely $\bar{H}$, such that $m(\mathbf{x}^*) \ne 0$, where $m = \det(\bar{H})$ and $\mathbf{x}^*$ is a solution of $\g$ when we work on the case $\deg(f_{i,j}) = D_i$ for all $1 \leq i \leq p, 1\leq j \leq q$. Let us recall the starting matrix $G$ in this case is 
\[ G = \left( \begin{matrix}
g_1 & 0 & \cdots & 0 & g_{1,p+1} & \cdots & g_{1, q}\\
0 & g_2 & \cdots & 0 & g_{2,p+1} & \cdots & g_{2, q}\\
\vdots & \vdots & \ddots & \vdots & \vdots & \ddots & \vdots\\
0 & 0 & \cdots & g_p & g_{p,p+1} & \cdots & g_{p, q}
\end{matrix} \right), \] where all $\{g_i\}_{1 \leq i \leq p}$ and $\{g_{k,j}\}_{1 \leq k \leq p, p+1\leq j \leq q}$ has generic coefficients. We have seen that for any solution $\mathbf{x}^{*}$ of $\g$, there is at least one $i \in \{1, \ldots , p\}$ such that $g_i(\mathbf{x}^*) = 0$. 

If there is only one $i \in \{1, \ldots, p\}$, without loss of generility we assume that $i=p$, such that $g_{p}(\mathbf{x}^*) = 0$ and $g_{j}(\mathbf{x}^*) \ne 0$ for $j \in \{1, \ldots, p-1\}$; then we define $\bar{G} = \mathrm{diag}(g_1, \ldots, g_{p-1})$. So, $\det(\bar{G})(\mathbf{x}^*) = \prod_{j=1}^{p-1}g_j(\mathbf{x}^*) \ne 0$.

If there are $k$ polynomials $g_{i_1}, \ldots, g_{i_k}$ for $(i_1, \ldots, i_k) \subset \{1, \ldots, p\}^k$ such that $g_{i_1}(\mathbf{x}^*) = \cdots = g_{i_k}(\mathbf{x}^*) = 0$ and $g_{i_j}(\mathbf{x}^*) \ne 0$ for $i_j \in \{1, \ldots, p\} \setminus \{i_1, \ldots, i_k\}$, we define $\bar{G} \in \field[\mat{X}]^{(p-1) \times (p-1)}$ as follows. Without loss of generity, let us work on $(i_1, \ldots, i_k) = (p-k+1, \ldots, p)$. For other tuples $(i_1, \ldots, i_k)$, we can use the similar argument. The idea is using these $g_{i_j}$ such that $g_{i_j}(\mathbf{x}^*) \ne 0$ to build the matrix $\bar{G}$. To sum up, we have $g_{p-k+1}(\mathbf{x}^*) = \cdots = g_{p}(\mathbf{x}^*) = 0$ and $g_j(\mathbf{x}^*) \ne 0$ for $1 \leq j \leq p-k$. Let us denote $A \in \field[\mat{X}]^{k \times (q-p)}$ for $G_{p-k+1:p\mathbf{;}p+1:q}$. Then, by using the genericity assumption $\mathcal{A}(1)$, there exists a submatrix ${G}^* \in \field[\mat{X}]^{(k-1) \times (k-1)}$ of $A$ such that $\det({G}^*)(\mathbf{x}^*) \ne 0$. We have finished the existence of the matrix ${G}^*$, we are going to find where is ${G}^*$ in $A$. 

Since ${G}^* \in \field[\mat{X}]^{(k-1) \times (k-1)}$  is a submatrix of $A$ such that $\det{{G}^*}(\mathbf{x}^*) \ne 0$, then ${G}^*(\mathbf{x}^*) \in \field^{(k-1)\times (k-1)}$ is a submatrix of $A(\mathbf{x}^*) \in \field^{k \times (q-p)}$ such that ${G}^*(\mathbf{x}^*)$ is full rank. Therefore, we can first, evaluate the matrix $A$ at the point $\mathbf{x}^*$ to obtain the matrix $A(\mathbf{x}^*) \in \field^{k \times (q-p)}$; and afterthat, thanks to Gaussian eliminations we can find a submatrix ${G}^*(\mathbf{x}^*)$ of $A(\mathbf{x}^*)$ such that $\mathrm{rank}({G}^*(\mathbf{x}^*)) = k-1$.  We obtain the matrix ${G}^* \in \field[\mat{X}]^{(k-1) \times (k-1)}$ with the indices are the same as those of ${G}^*(\mathbf{x}^*)$. Finally, let us define $\bar{G}$ as 
\[\bar{G} = 
\left[\begin{array}{c |c}%\hline
\mathrm{diag}(g_1, \ldots, g_{p-k}) & G_{1:p-k; \mathcal{J}} \\  \hline
\mat{O}_{k-1,p-k} & G^*\\
%\hline
\end{array}
\right] \in \field[\mat{X}]^{(p-1) \times (p-1)} \ ,\] where $\mathcal{J}$ is the column indices set of $G^*$. Then, $\det(\bar{G}) = \prod_{j = 1}^{p-k}g_j\det(G^*)$, for which we obtain $\det(\bar{G})(\mathbf{x}^*) \ne 0$.

For $\mathbf{x}^* \in V(\g)$, the remaining problem in this subsection is to verify that this square subsystem has the Jacobian matrix of full rank at ${\bf x}^*$. Let $I_{\mathcal{J}_k}$ be the ideal generated by $\{g_i\}_{p-k+1 \leq i \leq p}$ and all $k \times k$ minors of the matrix $G_{p-k+1:p;p+1:q}$. Then, 
$$V(I_{G_2}) \supset  V(I_{\mathcal{J}_k}) \ \mathrm{and} \ \mathbf{x}^* \in V(I_{\mathcal{J}_k}).$$
In addtion, from the genericity assumptions $\mathcal{A}_2$ and $\mathcal{A}_3$, all of $S_n(D_1, \dots, D_p)$ solutions are simple. So, all elements in $V(I_{\mathcal{J}_k})$ are simple, in particular ${\bf x}^*$ which is a solution of the square subsystem as defined in the previous paragraph. In other words, the Jacobian matrix of this system has full rank at ${\bf x}^*$. \todo{Todo: Check this problem}
\section{Homotopy techniques for determinantal systems} 
\label{sec:Algorithms}
Given a multivariate polynomial matrix $F \in \field[\mat{X}]^{p \times q}$ with the number of variables is $q-p+1$ and $p \leq q$; in this section, we build an algorithm by using the symbolic homotopy techniques to compute the isolated solutions of the determinantal system made by all $p \times p$ minors of $F$. 

Let us denote ${\bf{f}} = (f_1, \ldots, f_M) \in \field[{\mat{X}}]^M$ be the system of $p \times p$ minors of the input matrix $F \in \field[\mat{X}]^{p \times q}$ with $M = {{q}\choose{p}}$. Let $G \in \field[\mat{X}]^{p \times q}$ be a start matrix and $T$ is a new variable. Let us define the homotopy $H = (1-T)G + T.F \in \field[T,\mat{X}]^{p \times q}$. We denote ${\bf g} = (g_1, \ldots, g_M)$ in $\field[\mat{X}]^M$ and ${\bf h} = (h_1, \ldots, h_M)$ in $\field[T,\mat{X}]^M$ for the determiantal system of $G$ and $H$ respectively. 

\subsection{Testing a point is isolated}
\label{subsec:isolated}
Let $\mathbf{f}=(f_1,\dots,f_M)$ be polynomials in $\field[\mat{X}]$, with $\mat{X}=(X_1,\dots,X_n)$, for a field $\field$. Given a point $\mathbf{x}$ in $V(\mathbf{f})$, we discuss here how to decide whether $\mathbf{x}$ is an isolated point in $V(\mathbf{f})$. Without loss of generality, we assume that $M\ge n$; otherwise, $\mathbf{x}$
cannot be an isolated solution. In the determinantal system context, $M$ equals ${q \choose p}$, the number of $p\times p$ minors of $F$ and $M \ge n$. We make the following assumption (denoted by $\mathsf{H}$ below): we are given as input an integer $\mu$ such that
\begin{itemize}
\item either $\mathbf{x}$ belongs to a positive-dimensional component of $V(\mathbf{f})$,
\item or $\mathbf{x}$ is isolated in $V(\mathbf{f})$, with multiplicity at most $\mu$
  with respect to the ideal $\langle \mathbf{f} \rangle$.
\end{itemize}

\begin{Proposition}\label{testisolated} Suppose that $\mathbf{f}$ is given by a straight-line program of length $\mathcal{E}$. If assumption $\mathsf{H}$ is satisfied, we can decide whether $\mathbf{x}$ is an isolated root of $V(\mathbf{f})$ using $(\mu \,\mathcal{E} \,M)^{O(1)}$ operations in $\field$.
\end{Proposition}
\begin{proof}
The proof is given in \improve{\cref{sec:prooftestisolated}}. 
\end{proof}

\subsection{Computing the isolted solutions for determinantal systems}
Given polynomials ${\bf{f}} = (f_1, \ldots, f_M) \in \field[\mat{X}]^M$, we give an algorithm to compute a zero-dimensional parametrization of the isolated points of $V(\bf{f})$. All the notation of the previous subsection is still in use. In order to control the cost of the algorithm, we introduce the following assumptions:
\begin{description}
\item[${\sf A}_1$] Given ${\bf x} \in V(\h_0)$ having coordinates in a field extension $\mathbb{L}$ of $\field$, we can find in time $O\tilde{~}(B\, [\mathbb{L} : \field])$ a sequence ${\bm i}_{\mathbf{x}}= (i_1,\dots,i_n)$, with $1 \le i_1 < \dots < i_n \le M$, such that the Jacobian matrix of $(h_{0,i_1},\dots,h_{0,{i_n}})$ has full rank $n$ at ${\bf x}$, for some $B$ independent of ${\bf x}$.
\item[${\sf A}_2$] We know an integer $d$ such that the curve $V(J')$ has degree at most $d$. Lemma~\ref{lemma:vPi} implies that  $c \le d$.
\item[${\sf A}_3$] For any ${\bm i}=(i_1,\dots,i_n)$, with $1 \le i_1 <
  \dots < i_n \le M$, we can compute $(h_{i_1},\dots,h_{{i_n}})$ using
  a straight-line program of length $\mathcal{E}$.
%\item[${\sf A}_4$.] We can compute $\h$ using a straight-line program of length $\mathcal{E}' \ge \mathcal{E}$.
\end{description}

In the determinantal systems context, for the assumption ${\sf A}_1$, as we dicussed in \improve{Section~\ref{sec:extractsubsystem}}, given ${\bf x} \in V(\h_0)$, we can find ${\bm i}_{\bf x}$ in $\bigO{1}$ for the column degrees case; and for the row degrees case, $B$ is in fuction of evaluating point ${\bf x}$ in $\bar{G}$ and the complexity of Gaussian eliminations.  

\todo{Todo: Check assumptions ${\sf A}_2$, ${\sf A}_3$}
\paragraph{Starting points.} The algorithm starts by using the set of solution of the determiantal system of a start matrix which is defined as in \improve{Section~\ref{subsec:cd}}. We can use either Algorithm \ref{StartMatCol} for column degrees case or Algorithm \ref{StartMatRow} for the case of row degrees. We denote this set as $V(\g)$; and $|V(\g)| = \mathcal{T}$ with $\mathcal{T} = E_{n}(D_1, \ldots, D_q)$ in column degrees case  or $\mathcal{T} = S_{n}(D_1, \ldots, D_p)$ in row degrees case. For $j  =1, \ldots, \mathcal{T}$, from ${\sf A}_1 $, we can find ${\bm i}_{\bf x}$ such that $(h_{0,i})_{i \in {\bm i}_{\bf x}}$ has full rank $n$. By using the assumption ${\sf A}_1$, the runtime in this step is $(B \,\mathcal{T})^{\bigO{1}}$ . 

\paragraph{Lifting power series and rational reconstruction.} For $j  =1, \ldots, \mathcal{T}$, we apply the Newton interation to the system $(h_{0,i})_{i \in {\bm i}_{\bf x}}$ to lift ${\bf x}$ into a zero-dimensional parametrization $\mathscr{R}_j = ((q_j,v_{j,1}, \ldots, v_{j,n}), \lambda)$ with coefficients in $\field[[T]]/\langle T^{2d}\rangle$, for $d$ as in ${\sf A}_2$. 

As explained in~\cite[Section~2.2]{SaSc16}, using the algorithm of~\cite{GiLeSa01}, this can be done using $(d\,\mathcal{E}\,n)^{\bigO{1}}$ operations in $\field$. Using the Chinese Remainder Theorem, we can combine all $\mathscr{R}_j$ into a single zero-dimensional parametrization $\mathscr{R}$ with coefficients in $\field[[T]]/\langle T^{2d}\rangle$; this can be done in $(dn)^{\bigO{1}}$. 

Using the notation of the previous subsection and let $\Phi_1,\dots,\Phi_{c}$ be the points of $V(\mathfrak{J}')$, with coordinates taken in $\bar{\field}\langle\langle T \rangle\rangle$, then the zeros of $\mathscr{R}$ in $\bar{\field}[[T]]/\langle T^{2d} \rangle$ are truncations of $\Phi_1,\dots,\Phi_{c}$. Moreover, from ${\sf A}_2$, $J'$ is supposed to has degree at most $d$, knowing $\mathscr{R}$  at precision $2d$ allows us to reconstruct a zero-dimensional parametrization $\mathscr{S}$ with coefficients in $\field(T)$ such that the zeros of $\mathscr{S}$ is $V(\mathfrak{J}')$. This is done by applying rational function reconstruction to allcoefficients of $\mathscr{R}$, as in~\cite{Schost03}, and takes time $(d\,n)^{\bigO{1}}$. Without loss of generality, by multiplying all denominators of $\mathscr{R}$ coefficients in $\field(T)$ by the least common multiple of theirs, we can deduce that all polynomials of $\mathscr{R}$, say $q, v_1, \ldots, v_n$, have coefficients in $\field[T]$. The degree bounds in~\cite{Schost03} show that
if we require that $q, v_1, \ldots, v_n$ are in $\field[T][U]$ and without a common factor in $\field[T]$,
their total degrees are at most $d$, so this normalization can be
computed using $(d\,n)^{\bigO{1}}$ operations in $\field$. 
\paragraph{A finite set containing the isolated points of $V(\f)$.} Let us denote $J_1' = J + \langle T -1 \rangle$, then it can be seen in the proof of Lemma~\ref{lemma:19} that $V(J_1')$ is finite. In addition, Lemma~\ref{lemma:vPi} implies that for any isolated solution ${\bf x}$ of $\f$, $(1,{\bf x})$ is in $V(J'_1)$. So, we first deduce from $\mathscr{S}$ a zero-dimensional parametrization $\mathscr{R}_1$ with coefficients in $\field$
of $V(J'_1)$; and then by using the algorithm of \improve{Section~\ref{subsec:isolated}}, we discard from $V(J'_1)$ those points that do not correspond to isolated points of $V(\f)$. 

Let $\Phi'_1,\dots,\Phi'_c$ be the roots of $\mathfrak{J}'$ in the field of Puiseux series $\bar{\field}\langle\langle T'\rangle\rangle$ at $T=1$, where $T' = T-1$. Without loss of generality, we assume that
$\Phi'_1,\dots,\Phi'_\kappa$ are bounded, and
$\Phi'_{\kappa+1},\dots,\Phi'_c$ are not, for some $\kappa$ in
$\{0,\dots,c\}$, and we let $\varphi'_1,\dots,\varphi'_\kappa$ by
$\varphi'_i=\lim_0(\Phi'_i)\in\bar{\field}{}^n$ for
$i=1,\dots,\kappa$, where $\lim_0(\Phi'_i)$ is the coefficient of $T^0$ in $\varphi'_i$. Lemma~4.4 in~\cite{RRS} then shows how to recover a zero-dimensional parametrization
$\mathscr{R}_1=((q_1,v_{1,1},\dots,v_{1,n}),\lambda)$ with coefficients in
$\field$ for the limit set $\{\varphi'_i \mid i=1,\dots,\kappa\}$
starting from $\mathscr{S}$, under some conditions on the linear form
$\lambda$. As showed in~\cite{SaSc16}, these conditions are satisfied for a generic choice
of $\lambda$. Overall, $\mathscr{R}_1$ can be computed in time $(d\,n)^{\mathcal{O}(1)}$.

The following lemma shows that the parametrization $\mathscr{R}_1$ we just obtained 
describes $V(J' + \langle T' \rangle)=V(J' + \langle T-1 \rangle)$.

\begin{Lemma}\label{lemma:Z1}
  The equality $V(J' +\langle T' \rangle)=\{\varphi'_i \mid i=1,\dots,\kappa\}$ holds.
\end{Lemma}
\begin{proof}
The proof is given in \improve{\cref{sec:proofZ1}}. 
\end{proof}

\subsection{Algorithms.} We first give an algorithm called Algorithm~\ref{DetSys} to compute the isolated solutions set, $S$,  for the determinanal system $\f$ when knowing the solution set $V(\g)$ of the determinantal system of a start matrix. We denote $\mathscr{R}_{1}$ for a zero-dimensional parametrization with coefficients in $\field$ such that $S \subset \mathscr{R}_{1}$. Let $H \in \field[T, \mat{X}]^{p \times q}$ be defined as before, that is $H = (1 - T).G + T.F$. For each ${\bf x} \in V(\g)$, let $\h^{(\bf x)}$ in $\field[T,\mat{X}]^n$ a square subsystem such that $\h^{(\bf x)}(0,{\bf x}) = 0$ and the Jacobian matrix of $\h^{(\bf x)}$ has full rank $n$ at ${\bf x}$. Let us denote $d$ is either $E_{n}(D_1+1, \ldots, D_q+1)$ in column degrees case or $S_{n}(D_1+1, \ldots, D_p+1)$ in row degrees case. 

Through all sections of this report, we obtain the result as follows.
\begin{Theorem}
The Algorithm \ref{DetSys} is correct and its complexity is $\big({{q}\choose{p}} \,\mathcal{E}\,\mathcal{T}\big)^{\bigO{1}}$ operations in $\field$.
\end{Theorem}
\begin{algorithm}
\caption{$\mathsf{Determinantal System}$}
\label{DetSys}
{\bf Input}: a matrix $F \in \field[\mat{X}]^{p \times q}$ with $p \leq q$, the set $V(\g)$.\\
{\bf Output}: the isolated solutions set of $\f$. 
\begin{enumerate}
\item for any ${\bf x} \in V(\g)$: 
\begin{enumerate}
\item find a system $\h^{(\bf x)}$ in $\field[T,\mat{X}]^n$
\item compute $\mathscr{R}_{\bf x}$ zero-dimensional parametrization at precision $2d$. \\
$\mathsf{//* \ use \ \todo{algorithm???} \ in~\cite{GiLeSa01} \ }$
\end{enumerate}
\item combine $\{\mathscr{R}_{\bf x}\}_{{\bf x} \in V(\g)}$ into $\mathscr{R}$
\item compute $\mathscr{S}$ with coefficients in $\field(T)$ from  $\mathscr{R}$\\
$\mathsf{//* apply \ \todo{algorithm???} \ in~\cite{Schost03} \ to \ all \ coefficients \ of \ \mathscr{R} \ at \ degree} \ d$
\item clean denominators of $\mathscr{S}$
\item deduce $\mathscr{R}_{1}$ from $\mathscr{S}$
\item  $S \gets$ removing from $\mathscr{R}_{1}$ non-isolated points of $V(\f)$\\
$\mathsf{//* apply \ algorithm \ in~\cref{subsec:isolated}}$
\item return $S$
\end{enumerate}
\end{algorithm}
 
\begin{algorithm}
\caption{$\mathsf{ColumnDeterminantal System}$}
\label{CDetSys}
{\bf Input}: a matrix $F \in \field[\mat{X}]^{p \times q}$ with $p \leq q$ and $\deg(f_{i,j}) \leq D_j$ for all $1 \leq i \leq p$.\\
{\bf Output}: the isolated solutions set of $\f$. 
\begin{enumerate}
\item define a column start matrix $G \in \field[\mat{X}]^{p \times q}$ as in Section~\ref{subsec:cd}
\item $V(\g) \gets \mathsf{Start Matrix Column Degrees}(G)$
\item return $\mathsf{Determinantal System}(F, V(\g))$
\end{enumerate}
\end{algorithm}

\begin{algorithm}
\caption{$\mathsf{RowDeterminantal System}$}
\label{RDetSys}
{\bf Input}: a matrix $F \in \field[\mat{X}]^{p \times q}$ with $p \leq q$ and $\deg(f_{i,j}) \leq D_i$ for all $1 \leq j \leq q$.\\
{\bf Output}: the isolated solutions set of $\f$.
\begin{enumerate}
\item define a row start matrix $G \in \field[\mat{X}]^{p \times q}$ as in Section~\ref{subsec:cd}
\item $V(\g) \gets \mathsf{Start Matrix Row Degrees}(G)$
\item return $\mathsf{Determinantal System}(F, V(\g))$
\end{enumerate}
\end{algorithm}


In summary, if the input matrix is $F \in \field[\mat{X}]^{p \times q}$ such that $p \leq q$ and $\deg(f_{i,j}) \leq D_j$ for all $1 \leq i \leq p$, we will use the Algorithm~$\ref{CDetSys}$; and if the input matrix is $F \in \field[\mat{X}]^{p \times q}$ such that $p \leq q$ and $\deg(f_{i,j}) \leq D_i$ for all $1 \leq i \leq q$, we can use the Algorithm~\ref{RDetSys} for the Problem~\ref{problem}.
\subsection{Implementation}
\section{Conclusions and future research}
Given a field $\field$ and a matrix $F$ in multivariate polymomial ring $\field[\mat{X}]$, in this report, we design an algorithm to compute the set of points with coordinates in algebraically closed field $\bar{\field}$ such that at those points the matrix does not have full-rank.  Our algorithm mainly uses the symbolic homopoty approach. The runtime of this algorithm is \todo{Todo: runtime}. 

In the future, we would like to study how symbolic homotopy teachniques could be applied in the more general question, that is given a field $\field$, a matrix $F \in \field[\mat{X}]^{p \times q}$ and an integer $r \leq \min(p,q)$, compute the points in $\bar{\field}^n$, at which the rank of the matrix is at most $r$. %We also would like to study the polynomial systems solving with structures input systems, for instance, symetries, multi-homogeneity. 
\label{sec:conclusion}
\section{Acknowledgements}
\newpage
\bibliographystyle{plain}
\bibliography{biblio}
\appendix
\section{Proof of Propositon \ref{generic}}
\label{sec:proofgeneric}
Let $N = q(n+1)(\sum_{i=1}^pD_i)$ be the cardinality of the set $\mathcal{G}$. We notice that we are working on $n = q-p+1$, where $n$ is the number of variables. Let us define the matrices $G_1$ and $G_2$ as follow
\[G_1 = \left( \begin{matrix}
\mathfrak{g}_{1,1} & \mathfrak{g}_{1,2} & \cdots  & \mathfrak{g}_{1, q}\\
\mathfrak{g}_{2,1} & \mathfrak{g}_{2,2} & \cdots  & \mathfrak{g}_{2, q}\\
\vdots & \vdots & \ddots & \vdots \\
\mathfrak{g}_{p,1} & \mathfrak{g}_{p,2} & \cdots  & \mathfrak{g}_{p, q}
\end{matrix} \right) \ \mathrm{and} \ 
 G_2 = \left( \begin{matrix}
\mathfrak{g}_{1,1} & 0 & \cdots & 0 & \mathfrak{g}_{1,p+1} & \cdots & \mathfrak{g}_{1, q}\\
0 & \mathfrak{g}_{2,2} & \cdots & 0 & \mathfrak{g}_{2,p+1} & \cdots & \mathfrak{g}_{2, q}\\
\vdots & \vdots & \ddots & \vdots & \vdots & \ddots & \vdots\\
0 & 0 & \cdots & \mathfrak{g}_{p,p} & \mathfrak{g}_{p,p+1} & \cdots & \mathfrak{g}_{p, q}
\end{matrix} \right). \] 

The idea to prove $\mathcal{A}(k)$, for any $k \in \{1,2,3\}$, is by using induction on the number of variables, $n$, for the matrix of form $G_1$; and, after that we show that the property $\mathcal{A}(k)$ is true for the matrix $G_2$. Finally, by using this property of form $G_2$, we prove  $\mathcal{A}(k)$ holds for any matrix of form $G_1$. In this section, given a polynomial matrix $G$ and an integer $k < p$, we denote $I_G$ for the ideal generated by all $p \times p$ minors of $G$ and $I_G^{(k)}$ for the ideal generated by all $k \times k$ minors of $G$. 
\subsection{Genericity of $\mathcal{A}(1)$}
In this section, we use the fact that for any $\mathbf{x}^* \in V(I_G)$, the rank of the matrix $G(\mathbf{x}^*)$ is exactly $p-1$ if $\mathbf{x}^*$ does not belong to the variety of the ideal generated by $(p-1)\times (p-1)$ minors of $G$. Indeed, if $\mathbf{x}^* \in V(I_{G}^{(p-1)})$ where $I_{G}^{(p-1)} = \langle (p-1) \times (p-1) -\mathrm{minors \ of} \ G \rangle$, then $\mathrm{rank}(G(\mathbf{x}^*)) \leq p-2$.


$\bf{Step \ 1:}$ First, we will prove that there exist a nonempty Zariski open set  $\mathcal{O}_1$ such that $I_{G_1}$ satisfies $\mathcal{A}(1)$ for $g_{i,j} \in\mathcal{O}_1$, when $G_1$ is a square matrix as follows
\[
G_1 = \left( \begin{matrix}
\mathfrak{g}_{1,1}  & \cdots  & \mathfrak{g}_{1, p}\\
\vdots & \ddots & \vdots \\
\mathfrak{g}_{p,1} & \cdots  & \mathfrak{g}_{p, p}
\end{matrix} \right).
\] We notice that in this case, the number of variables is $n=1$ and the number of coefficients is $N = 2q.(\sum_{i=1}^pD_i)$. Let us define $I_{G_1}^{(p-1)} := \langle (p-1) \times (p-1) - \mathrm{minors \ of \ } G_1 \rangle \subset  \field[\mathcal{G}, \mat{X}]$ and $\Omega_1 = V(I_{G_1}^{(p-1)}) \subset \bar{\field}^{N} \times \mathbb{P}^{1}(\bar{\field})$. Then, $\Omega_1$ is a Zariski closed in $\bar{\field}^{N} \times \mathbb{P}^{1}(\bar{\field})$. Let $\pi_{\mathcal{G}} : \bar{\field}^{N} \times \mathbb{P}^{1}(\bar{\field}) \to \bar{\field}^{N}$ be the projection on the $\mathcal{G}$ coordinates and $\Delta_1 = \pi_{\mathcal{G}}(\Omega_1)$. This implies $\Delta_1$ is a Zariski closed in $\bar{\field}^{N}$. So, we define the Zariski open set $\mathcal{O}_1$ in $\bar{\field}^{N}$ as $\mathcal{O}_1 := \bar{\field}^{N} \setminus \Delta_1$. We claim that $\mathcal{O}_1$ is a nonempty set. Indeed, let $G_2 = \mathrm{diag}(\mathfrak{g}_{1,1}, \ldots, \mathfrak{g}_{1,1})$ be a diagonal matrix and $I_{G_2}^{(p-1)} := \langle (p-1) \times (p-1) - \mathrm{minors \ of \ } G_2 \rangle \subset  \field[\mathcal{G}, \mat{X}]$. Therefore, for any $f_{G_2} \in I_{G_2}^{(p-1)}$, $f_{G_2}$ has the form $\mathfrak{g}_{i_1, i_1} \ldots \mathfrak{g}_{i_{p-1}, i_{p-1}}$ where $(i_1, \ldots, i_{p-1}) \in \{1, \ldots, p\}^{p-1}$. Moreover, for any $1 \leq i \leq p$, we have $\mathfrak{g}_{i,i} = (\gamma_{i,i}^{(1,1)}X_1 + \gamma_{i,i}^{(0,1)}) \times \cdots \times (\gamma_{i,i}^{(1,D_i)}X_1 + \gamma_{i,i}^{(0,D_i)})$. So, for any $(\mathfrak{g}^*, \mathbf{x}^*) \in V(I_{G_2}^{(p-1)})$, it is the solution of $\gamma_{i,i}^{(1,t)}X_1 + \gamma_{i,i}^{(0,t)}X_0 = 0$ for $0 \leq t \leq D_i$. This equation always has solution in $\bar{\field}^{2} \times \mathbb{P}^{1}(\bar{\field})$. This implies, if we define $\bar{\Omega}_1 := V(I_{G_2}^{(p-1)}) \in \bar{\field}^{N_2} \times \mathbb{P}^{1}(\bar{\field})$,  $\bar{\Omega}_1$ will be a Zariski closed in $\bar{\field}^{N_2} \times \mathbb{P}^{1}(\bar{\field})$, where $N_2 = 2.(\sum_{i = 1}^pD_i)$ is the number of generic coefficients for $G_2$. Finally, let us define $\bar{\Delta}_1$ as $\pi_{\bar{\mathcal{G}}}(\bar{\Omega}_1)$, where $\pi_{\bar{\mathcal{G}}} : \bar{\field}^{N_2} \times \mathbb{P}^{1}(\bar{\field}) \to \bar{\field}^{N_2}$; and $\bar{\mathcal{O}}_1 = \bar{\Delta}_1\times 0^{N_1}$ which is nonempty set, where $N_1 = N - N_2$; and it can be saw that 
$\bar{\mathcal{O}}_1 \subset \mathcal{O}_1$. As a consequence,  $\mathcal{O}_1$ is nonempty. We finished the first step of induction. 

$\bf{Step \ 2:}$ Let us now assume that $\mathcal{A}(1)$ holds for any $G_1 \in \field[{\mathcal{G}, \mat{X}}]^{p' \times q'}$ for any $n' \leq n$, where $n' = q'-p'+1$; we will prove $\mathcal{A}(1)$ is also true for $G_2 \in \field[\mathcal{G}, \mat{X}]^{p \times q}$ with $n = q-p+1$. Noting that $I_{G_2}^{(p-1)}$ contains $g_{i_1, i_1}\ldots g_{i_{p-1}, i_{p-1}}$ for any $(i_1, \ldots, i_{p-1}) \in \{1, \ldots, p\}^{p-1}$ and $i_j \ne i_t$. For any $(\mathfrak{g}^*, \mathbf{x}^*) \in (\bar{\field}^{N_2} \times \mathbb{P}^{n}(\bar{\field})) \cap V(I_{G_2}^{(p-1)})$, there are at least two $1\leq j,t \leq p, j \ne t$ such that $\mathfrak{g}_{j,j}(\mathfrak{g}^*, \mathbf{x}^*) = \mathfrak{g}_{t,t}(\mathfrak{g}^*, \mathbf{x}^*) = 0$. If there are $k$ numbers $i_1, \ldots, i_k$, where $2 \leq k \leq \min(n,p)$, such that $g_{i_1, i_1}(\mathfrak{g}^*, \mathbf{x}^*) = \cdots = g_{i_k, i_k}(\mathfrak{g}^*, \mathbf{x}^*) = 0$, we define $\mathcal{J}_k = (i_1, \ldots, i_k)$ and $G_{\mathcal{J}_k, p+1:q} \in \field[\mathcal{G}, \mat{X}]^{(n-k)\times (q-p)}$ is the submatrix of $G_2$ that contains the rows $\mathcal{J}_k$ and columns $p+1, \ldots, q$. By using the similar argument as in Proposition \ref{r2}, we have $\mathrm{rank}(G_{\mathcal{J}_k, p+1:q}(\mathfrak{g}^*, \mathbf{x}^*)) = k-1$ for which the property $\mathcal{A}(1)$ holds as from induction hypothesis. Moreover, $$V(I_{G_2}^{(p-1)}) = \bigcup\limits_{\mathcal{J}_k \subset\{1, \ldots, p\}^k, \ 2 \leq k \leq \min(n,p)} V(I_{\mathcal{J}_k}),$$ where $I_{\mathcal{J}_k}$ is the ideal generated by $\mathfrak{g}_{i,i}$ for $i \in \mathcal{J}_k$ and all $(k-2)\times(k-2) -$ minors of $G_{\mathcal{J}_k, p+1:q}$. Therefore, we can define a nonempty Zariski open, $\bar{\mathcal{O}}_1$, for $G_2$ as follows. For any $\mathcal{J}_k \subset \{1, \ldots, p\}^k$, we rewrite $(X_1, \ldots, X_k) \in \field[\mathcal{G}, X_{k+1}, \ldots, X_n]^{k}$ and subtitute into $G_{\mathcal{J}_k, p+1:q}$ to obtain $G_{\mathcal{J}_k, p+1:q} \in \field[\mathcal{G}, X_{k+1}, \ldots, X_n]^{k \times {(q-p)}}$. By using the induction hypothesis, for any $\mathcal{J}_k$, there exists a Zariski closed set $\Delta_{\mathcal{J}_k} \subsetneq \bar{\field}^{N_{\mathcal{J}_k}}$, where $N_{\mathcal{J}_k} = n(q-p+1)(\sum_{i \in \mathcal{J}_k}D_i)$ is the number of coefficients of the matrix $[\mathrm{diag}(g_{i_1, i_1}, \ldots, g_{i_k, i_k})|G_{\mathcal{J}_k, p+1:q}]$. %$\Omega_{\mathcal{J}_k} \subset \bar{\field}^{N_{\mathcal{J}_k}} \times \mathbb{P}^{n-k+1}(\bar{\field})$ be the variety of the ideal $\langle (k-1) \times (k-1) - \mathrm{minors \ of \ } G_{\mathcal{J}_k, p+1:q} \rangle$, where $N_{\mathcal{J}_k} = n(q-p+1)(\sum_{i \in \mathcal{J}_k}D_i)$ is the number of coefficients of the matrix $[\mathrm{diag}(g_{i_1, i_1}, \ldots, g_{i_k, i_k})|G_{\mathcal{J}_k, p+1:q}]$. We define $\Delta_{\mathcal{J}_k}$ as $\pi_{\mathcal{G}_{\mathcal{J}_k}}(\Delta_{\mathcal{J}_k})$, where $$\mathcal{G}_{\mathcal{J}_k} :  \bar{\field}^{N_{\mathcal{J}_k}} \times \mathbb{P}^{n-k+1}(\bar{\field}) \to \bar{\field}^{N_{\mathcal{J}_k}}.$$ 
We finally, define \[\bar{\mathcal{O}}_1 = \bar{\field}^{N_2} \setminus \Delta, \ \mathrm{where}\  N_2 = n(q-p+1)(\sum_{i=1}^pD_i) \ \mathrm{and} \ \Delta = \bigcup_{\mathcal{J}_k \subset\{1, \ldots, p\}^k, \ 2 \leq k \leq \min(n,p)}\Delta_{\mathcal{J}_k}.\]
This is a nonempty Zariski open set in $\bar{\field}^{N_2}$ such that $I_{G_2}$ satisfies $\mathcal{A}(1)$ when $g_{i,j} \in \bar{\mathcal{O}}_1$. This means we finished the second step of the proof. 

$\bf{Step \ 3:}$ Finally, let assume that $\mathcal{A}(1)$ holds for any $G_2 \in \field[\mathcal{G}, \mat{X}]^{p \times q}$, we will prove $\mathcal{A}(1)$ is also true for any $G_1 \in \field[\mathcal{G}, \mat{X}]^{p \times q}$ as follows. Let us define $I_{G_1}^{(p-1)} = \langle (p-1) \times (p-1) - \mathrm{minors \ of} \ G_1 \rangle$ and $\Omega_1 = V(I_{G_1}^{(p-1)}) \subset \bar{\field}^{N} \times \mathbb{P}^n(\bar{\field})$ is a Zariski closed. Similarly as in the first step, we define $\Delta_1 = \pi_{\mathcal{G}}(\Omega_1)$ is a Zariski closed in $\bar{\field}^{N}$, where $\pi_{\mathcal{G}} : \bar{\field}^{N} \times \mathbb{P}^n(\bar{\field}) \to \bar{\field}^{N}$ is a projection. So, we define the Zariski open set $\mathcal{O}_1$ in $\bar{\field}^{N}$ as $\mathcal{O}_1 := \bar{\field}^{N} \setminus \Delta_1$. By using the matrix $G_1$ of the form as in the second step and similar argument as the first step, we can deduce that $\mathcal{O}_1$ is nonempty. 

We finished the proof for the property $\mathcal{A}(1)$. 
\subsection{Genericity of $\mathcal{A}(2)$}
$\bf{Step \ 1:}$ We show here that there exists a nonempty Zariski open set $\mathcal{O}_2$ such that $I_{G_1}$ has $S_1(D_1, \ldots, D_p)$ (that is $\sum_{i=1}^pD_i$) distinct solutions for $\mathfrak{g}_{i,j} \in \mathcal{O}_2$, when $G_1$ is a square matrix as follows
\[
G_1 = \left( \begin{matrix}
\mathfrak{g}_{1,1}  & \cdots  & \mathfrak{g}_{1, p}\\
\vdots & \ddots & \vdots \\
\mathfrak{g}_{p,1} & \cdots  & \mathfrak{g}_{p, p}
\end{matrix} \right).
\] We notice that in this case, each $\mathfrak{g}_{i,j}$ has the form $\mathfrak{g}_{i,j} = (\gamma_{i,j}^{(1,1)}X_1 + \gamma_{i,j}^{(0,1)}) \times \cdots \times (\gamma_{i,j}^{(1,D_i)}X_1 + \gamma_{i,j}^{(0,D_i)})$. Let us denote $m \in \field[\mathcal{G}, X_1]$ for the deteminant of $G_1$, then $m$ is homogeneous in $\mathcal{G}$ of degree $p$ and $\deg_X(m) = \sum_{i=1}^pD_i$. Indeed, the coefficient of $X_1^{d}$, where $d = \sum_{i=1}^pD_i$, in $m$ is $\det(A)$, where 
\[
A = \left( \begin{matrix}
\prod_{l=1}^{D_1}\gamma_{1,1}^{(1,l)}  & \cdots  & \prod_{l=1}^{D_1}\gamma_{1,p}^{(1,l)}\\
\vdots & \ddots & \vdots \\
\prod_{l=1}^{D_p}\gamma_{1,1}^{(1,l)}  & \cdots  & \prod_{l=1}^{D_p}\gamma_{1,p}^{(1,l)}\\
\end{matrix} \right) \in \field[\mathcal{G}]^{p \times p}.
\] which has nonzero determinant. Moreover, in order to obtain the condition that $m$ has $\sum_{i=1}^pD_i$ distinct solutions we use $\mathrm{Res}_{X_1}(m, m') \ne 0$, where $\mathrm{Res}_{X_1}(m, m')$ is the resultant between $m$ and the derivative of $m$ with respect to $X_1$. So, we can define $\mathcal{O}_2$ as follows. 

Let $\Omega_2 = V(\mathrm{Res}_{X_1}(m, m')) \subset \bar{\field}^{N} \times \mathbb{P}^1(\bar{\field})$ is a Zariski closed, where here $N = 2p(\sum_{i=1}^pD_i)$ is the number of coefficients for $G_1$. Then, we define $\Delta_2 = \pi_{\mathcal{G}}(\Omega_2) \subset \bar{\field}^{N}$ is a Zariski closed and take $\mathcal{O}_2$ as $\bar{\field}^{N} \setminus \Delta_2$ is a Zariski open. Therefore, for any $\mathfrak{g}_{i,j} \in \mathcal{O}_2$, we have $m$ has $\sum_{i=1}^pD_i$ solutions. The remaining problem is we need to check that $\mathcal{O}_2$ is nonempty. In order to finish this part, we use the matrix $G_2$ as a diagonal matrix $\mathrm{diag}(\mathfrak{g}_{1,1}, \ldots, \mathfrak{g}_{p,p})$ where $\mathfrak{g}_{i,i} = \prod_{l=1}^{D_i}(\gamma_{i,i}^{(1,l)}X_1 + \gamma_{i,i}^{(0,l)})$. Furthermore, the determinat of $G_2$ has the form $\prod_{i=1}^p\mathfrak{g}_{i,i}$, i.e., $\prod_{i=1}^p\prod_{l=1}^{D_i}(\gamma_{i,i}^{(1,l)}X_1 + \gamma_{i,i}^{(0,l)})$. Then, $\det(G_2)$ always has $\sum_{i=1}^pD_i$  distinct solutions. Therefore, $\bar{\field}^{N_2} \times 0^{N_1} \subset \mathcal{O}_2$ which implies that $\mathcal{O}_2$ is nonempty. 

$\bf{Step \ 2:}$ Let us now assume that $\mathcal{A}(2)$ holds for any $G_1 \in \field[{\mathcal{G}, \mat{X}}]^{p' \times q'}$ for any $n' \leq n$, where $n' = q'-p'+1$; we will prove $\mathcal{A}(2)$ is also true for $G_2 \in \field[\mathcal{G}, \mat{X}]^{p \times q}$ with $n = q-p+1$. We follow the similar argument as in the second step of the proof for $\mathcal{A}(1)$. Here, instead of consider the $I_{G_2}^{(p-1)}$, we consider the ideal $I_{G_2}$ which is generated by $p \times p$ minors of $G_2$. We have $I_{G_2}$ contains $\mathfrak{g}_{1,1}\ldots \mathfrak{g}_{p,p}$. Then, for any solution, $(\mathfrak{g}^*,\mathbf{x}^*) \in \bar{\field}^{N_2} \times \mathbb{P}^n(\bar{\field})$ of $I_{G_2}$, there is at least one $i \in \{1, \ldots, p\}$ such that $\mathfrak{g}_{i,i}(\mathfrak{g}^*,\mathbf{x}^*) = 0$. If there are $k$ numbers $i_1, \ldots, i_k$, where $1 \leq k \leq \min(n,p)$, such that $g_{i_1, i_1}(\mathfrak{g}^*, \mathbf{x}^*) = \cdots = g_{i_k, i_k}(\mathfrak{g}^*, \mathbf{x}^*) = 0$, we define $\mathcal{J}_k = (i_1, \ldots, i_k)$ and $G_{\mathcal{J}_k, p+1:q} \in \field[\mathcal{G}, \mat{X}]^{(n-k)\times (q-p)}$ is the submatrix of $G_2$ that contains the rows $\mathcal{J}_k$ and columns $p+1, \ldots, q$. From Proposition \ref{r2}, we have $\mathrm{rank}(G_{\mathcal{J}_k, p+1:q}(\mathfrak{g}^*, \mathbf{x}^*)) \leq k-1$ for which the property $\mathcal{A}(2)$ holds as from induction hypothesis. Moreover, $$V(I_{G_2}) = \bigcup\limits_{\mathcal{J}_k \subset\{1, \ldots, p\}^k, \ 1 \leq k \leq \min(n,p)} V(I_{\mathcal{J}_k}),$$ where $I_{\mathcal{J}_k}$ is the ideal generated by $\mathfrak{g}_{i,i}$ for $i \in \mathcal{J}_k$ and all $(k-1)\times(k-1)$ minors of $G_{\mathcal{J}_k, p+1:q}$. Therefore, we can define a nonempty Zariski open, $\bar{\mathcal{O}}_2$, for $G_2$ as follows. For any $\mathcal{J}_k \subset \{1, \ldots, p\}^k$, we rewrite $(X_1, \ldots, X_k) \in \field[\mathcal{G}, X_{k+1}, \ldots, X_n]^{k}$ and subtitute into $G_{\mathcal{J}_k, p+1:q}$ to obtain $G_{\mathcal{J}_k, p+1:q} \in \field[\mathcal{G}, X_{k+1}, \ldots, X_n]^{k \times {(q-p)}}$. By using the induction hypothesis, for any $\mathcal{J}_k$, there exists a Zariski closed set $\Delta_{\mathcal{J}_k} \subsetneq \bar{\field}^{N_{\mathcal{J}_k}}$ such that $I_{G_{\mathcal{J}_k, p+1:q}}^{(p)}$ has $S_{n-k}(D_{i_1}, \ldots, D_{i_k})$ distinct solutions, where $N_{\mathcal{J}_k} = n(q-p+1)(\sum_{i \in \mathcal{J}_k}D_i)$ is the number of coefficients of the matrix $[\mathrm{diag}(\mathfrak{g}_{i_1, i_1}, \ldots, \mathfrak{g}_{i_k, i_k})|G_{\mathcal{J}_k, p+1:q}]$. We finally, define \[\bar{\mathcal{O}}_2 = \bar{\field}^{N_2} \setminus \Delta, \ \mathrm{where}\  N_2 = n(q-p+1)(\sum_{i=1}^pD_i) \ \mathrm{and} \ \Delta = \bigcup_{\mathcal{J}_k \subset\{1, \ldots, p\}^k, \ 1 \leq k \leq \min(n,p)}\Delta_{\mathcal{J}_k}.\]
This is a nonempty Zariski open set in $\bar{\field}^{N_2}$ such that $I_{G_2}$ satisfies $\mathcal{A}(2)$ when $g_{i,j} \in \bar{\mathcal{O}}_2$. Indeed, by this construction, the number of solutions of $I_{G_2}$ equals

\[\sum_{k=1}^{\min(n,p)} \sum_{\mathcal{J}_k \subset\{1, \ldots, p\}^k}D_{i_1} \ldots D_{i_k} . \# \{\mathrm{solutions \ of \ } I_{G_{\mathcal{J}_k, p+1:q}}\}\] which is 
\[
\sum_{k=1}^{\min(n,p)} \sum_{\mathcal{J}_k \subset\{1, \ldots, p\}^k}D_{i_1} \ldots D_{i_k}S_{n-k}(D_{i_1}, \ldots, D_{i_k}) = S_n(D_1, \ldots, D_p). 
\]

$\bf{Step \ 3:}$ Finally, let assume that $\mathcal{A}(2)$ holds for any $G_2 \in \field[\mathcal{G}, \mat{X}]^{p \times q}$, we will prove $\mathcal{A}(2)$ is also true for any $G_1 \in \field[\mathcal{G}, \mat{X}]^{p \times q}$ as follows. We finish this step by using the result in \improve{Section~\ref{sec:bounddegree}}. Indeed, we can use the fact that $I_{G_1}$ satisfis ${\sf G}$; and consider the matrices $G_1$ and $G_2$ here as the matrices $F$ and $G$ in \improve{Section~\ref{sec:bounddegree}}, respectively. In other words, if $G_2$ has $S_n(D_1, \ldots, D_p)$ solutions, then so is $G_1$. Therefore, we can use the Zariski open set $\bar{\mathcal{O}}_2$ in the second step as ${\mathcal{O}_2}$ for $G_1$. This set is indeed nonempty from the second step above. 
\subsection{Genericity of $\mathcal{A}(3)$} 
$\bf{Step \ 1:}$ We show here that there exists a nonempty Zariski open set $\mathcal{O}_3$ such that $I_{G_1}$ is radical ideal for $\mathfrak{g}_{i,j} \in \mathcal{O}_3$, where $G_1$ is as square matrix as follows 
\[
G_1 = \left( \begin{matrix}
\mathfrak{g}_{1,1}  & \cdots  & \mathfrak{g}_{1, p}\\
\vdots & \ddots & \vdots \\
\mathfrak{g}_{p,1} & \cdots  & \mathfrak{g}_{p, p}
\end{matrix} \right).
\] Let us denote $m \in \field[\mathcal{G}, X_1]$ for the deteminant of $G_1$, then $I_{G_1} = \langle m \rangle$. Notice that in this case, the number of variable is $n = 1$. We would like to have the property that \[\frac{\partial m}{\partial X_1}(\mathbf{x}^*) \ne 0 \ \mathrm{for \ any} \ \mathbf{x}^* \in V(m). \] 

Let us define $\mathcal{O}_3$ as follows. Let $\Omega_3 = V(\partial m / \partial X_1) \subset \bar{\field}^N \times \mathbb{P}^1(\bar{\field})$ is a Zariski closed, where here $N = 2p(\sum_{i=1}^pD_i)$ is the number of coefficients for $G_1$. Then, we define $\Delta_3 = \pi_{\mathcal{G}}(\Omega_3) \subset \bar{\field}^{N}$ is a Zariski closed and take $\mathcal{O}_3$ as $\bar{\field}^{N} \setminus \Delta_3$ is a Zariski open. Therefore, for any $\mathfrak{g}_{i,j} \in \mathcal{O}_3$, we have $(\partial m / \partial X_1) (\mathbf{x}^*) \ne 0$. The remaining problem is we need to check that $\mathcal{O}_3$ is nonempty. For this, we use the diagonal marix $G_2$ as $\mathrm{diag}(\mathfrak{g}_{1,1}, \ldots, \mathfrak{g}_{p,p})$. So, the determinant of $G_2$ is $\prod_{i=1}^p\mathfrak{g}_{i,i}$, where $\mathfrak{g}_{i,i} = \prod_{l=1}^{D_i}(\gamma_{i,i}^{(1,l)}X_1 + \gamma_{i,i}^{(0,l)})$. Let us denote $u$ for $\det(G_2)$, then 
\[
\frac{\partial u}{\partial X_1} = \sum_{i = 1}^p\frac{\partial \mathfrak{g}_{i,i}}{\partial X_1}\prod_{j \ne i} \mathfrak{g}_{j,j}.
\]
For convernient, let us denote $f(X)$ for ${\partial u}/{\partial X_1}$ and rewrite $\det(G_2) = \prod_{i=1}^d(a_iX + b_i)$ for $a_i$ and $b_i$ are indeterminantes and $d = \sum_{i=1}^pD_i$. Therefore, 
\[
f(X) = \sum_{i = 1}^d a_i \prod_{j=1, j \ne i}^d(a_jX + b_j).
\]
Moreover, for any $\mathbf{x}^* \in V(\det(G_2))$ and $\mathbf{x}^* $ is the solution of $a_i X+ b_i = 0$, we have $a_j \mathbf{x}^* + b_j \ne 0$ for all $i \ne k$. Therefore, \[f(\mathbf{x}^*) = a_i\prod_{j=1, j \ne i}^d(a_j\mathbf{x}^* + b_j) \ne 0. \]
This means for any $\mathbf{x}^* \in V(\det(G_2))$, we always have $f(\mathbf{x}^*) \ne 0$. Thus, $\bar{\field}^{N_2} \times 0^{N_1} \subset \mathcal{O}_3$ which implies that $\mathcal{O}_3$ is nonempty. 

$\bf{Step \ 2:}$ Let us now assume that $\mathcal{A}(3)$ holds for any $G_1 \in \field[{\mathcal{G}, \mat{X}}]^{p' \times q'}$ for any $n' \leq n$, where $n' = q'-p'+1$; we will prove $\mathcal{A}(3)$ is also true for $G_2 \in \field[\mathcal{G}, \mat{X}]^{p \times q}$ with $n = q-p+1$. As we noticed before, the assumption that $I_{G_1}$ is radical is equivalent to the property that $\mathrm{Jac}(I_{G})(\mathbf{x}^*)$ has full rank for any $\mathbf{x}^* \in V(I_{G})$. In other words, $\mathbf{x}^* \in V(I_{G})$ is a simple root in $V(I_G)$. 

Let us now consider any $\mathbf{x}^* \in V(I_{G_2})$, there exist $k$ polynomials $\mathfrak{g}_{i_1,i_1}, \ldots, \mathfrak{g}_{i_k,i_k}$ such that $\mathfrak{g}_{i_1,i_1}(\mathbf{x}^*) = \cdots = \mathfrak{g}_{i_k,i_k}(\mathbf{x}^*) = 0$, where $1 \leq k \leq \min(n,p)$ and $\mathrm{rank}(G_{\mathcal{J}_k; p+1:q}(\mathbf{x}^*)) \leq k-1$. That is $\mathbf{x}^*$ is solution of the system of $k \times k -$ minors of $G_{\mathcal{J}_k; p+1:q}$, where $\mathcal{J}_k = (i_1, \ldots, i_k)$. Therefore, by considering any system of $k$ polynomials $\mathfrak{g}_{i_1,i_1}(\mat{X}) = \cdots = \mathfrak{g}_{i_k,i_k}(\mat{X}) = 0$, we can rewrite the variables $X_1, \ldots, X_k$ in the linear of $X_{k+1}, \ldots, X_n$; then by substitute $\{X_i\}_{i=1}^{k}$ in to $G_{\mathcal{J}_k; p+1:q}$ we obtain a matrix $\bar{G} \in \field[\mathcal{G}, X_k, \ldots, X_n]^{k \times (q-p)}$. Furthermore, this $\bar{G}$ matrix has property that any solution of the its $k \times k -$ minors system is simple by using induction hypothesis. Therefore, we can define the Zariski open, $\bar{\mathcal{O}}_3$, for $G_2$ as follows. 

For any $\mathcal{J}_k \subset \{1, \ldots, p\}^k$, we rewrite $(X_1, \ldots, X_k) \in \field[\mathcal{G}, X_{k+1}, \ldots, X_n]^{k}$ and subtitute into $G_{\mathcal{J}_k, p+1:q}$ to obtain $G_{\mathcal{J}_k, p+1:q} \in \field[\mathcal{G}, X_{k+1}, \ldots, X_n]^{k \times {(q-p)}}$. By using the induction hypothesis, for any $\mathcal{J}_k$, there exists a Zariski closed set $\Delta_{\mathcal{J}_k} \subsetneq \bar{\field}^{N_{\mathcal{J}_k}}$ such that any solution of $k \times k -$ minors of $G_{\mathcal{J}_k, p+1:q}$ is simple, where $N_{\mathcal{J}_k} = n(q-p+1)(\sum_{i \in \mathcal{J}_k}D_i)$ is the number of coefficients of the matrix $[\mathrm{diag}(\mathfrak{g}_{i_1, i_1}, \ldots, \mathfrak{g}_{i_k, i_k})|G_{\mathcal{J}_k, p+1:q}]$. We finally, define \[\bar{\mathcal{O}}_3 = \bar{\field}^{N_2} \setminus \Delta, \ \mathrm{where}\  N_2 = n(q-p+1)(\sum_{i=1}^pD_i) \ \mathrm{and} \ \Delta = \bigcup_{\mathcal{J}_k \subset\{1, \ldots, p\}^k, \ 1 \leq k \leq \min(n,p)}\Delta_{\mathcal{J}_k}.\]
This is a nonempty Zariski open set in $\bar{\field}^{N_2}$ such that $I_{G_2}$ satisfies $\mathcal{A}(3)$ when $g_{i,j} \in \bar{\mathcal{O}}_3$. Indeed, by this construction, from the linear of $X_1, \ldots, X_k$ in $X_{k+1}, \ldots, X_n$ and any solution of $k \times k -$ minors of $G_{\mathcal{J}_k, p+1:q}$ is simple, we can deduce that any solution of $p \times p -$ minors of $G_2$ is simple. It means we finishes the second step.

$\bf{Step \ 3:}$
Let us consider the matrix $G_1 \in \field[\mathcal{G}, \mat{X}]^{p \times q}$, and $I_{G_1} := \langle p \times p - \ \mathrm{minors \ of } \ G_1 \rangle$. Let $m_1, \ldots, m_d$ be the generators for $I_{G_1}$, where $d = {{q}\choose{p}}$ is the number of $p \times p$ minors of $G_1$. We would like to have the property that $\mathrm{Jac}(I_{G_1})(\mathbf{x}^*)$ has full rank for any $\mathbf{x}^* \in V(I_{G_1})$ . We recall here that $$\mathrm{Jac}(I_{G_1}) = \left[\frac{\partial m_j}{\partial X_i}\right]_{1 \leq j \leq d, 1 \leq i \leq n} \in \field[\mathcal{G}, \mat{X}]^{d \times n}. $$
We first notice that for since $n \leq d$, so for any $\mathbf{x}^* \in \bar{\field}^n$, the matrix $\mathrm{Jac}(I_{G_1})(\mathbf{x}^*)$ has rank at most $n$. Therefore, $\mathrm{Jac}(I_{G_1})(\mathbf{x}^*)$ should has rank $n$ if we want the property that $\mathrm{Jac}(I_{G_1})$ has full rank at any point $\mathbf{x}^*$. For this argument, we consider the ideal which is generated by $n \times n \ \mathrm{minors \ of} \ \mathrm{Jac}(I_{G_1})$, that is $\mathcal{I}_{m} = \langle n \times n  \ \mathrm{minors \ of} \ \mathrm{Jac}(I_{G_1}) \rangle$.

Let us define $\Omega_3 = V(\mathcal{I}_m) \subset \bar{\field}^N \times \mathbb{P}^n(\bar{\field})$ is a Zariski closed. Let $\pi_{\mathcal{G}}$ be the projection on $\mathcal{G}$ coordinates, then we define $\Delta_3 = \pi_{\mathcal{G}}(\Omega_3)$ is a Zariski closed in $\bar{\field}^N $. Finally, let us denote  $\mathcal{O}_3$ for $\bar{\field}^N \setminus \Delta_3$ which is a Zariski open in $\bar{\field}^N $. By using this contruction, when $\mathfrak{g}_{i,j} \in \mathcal{O}_3$, we can see that  $\mathrm{Jac}(I_{G_1})(\mathbf{x}^*)$ has full rank $n$ for any $\mathbf{x}^* \in V(I_{G_1})$). The remaining problem is proving $\mathcal{O}_3$ is indeed a nonempty set. In order to finish this step, we use the matrix $G_2 \in \field[\mathcal{G}, \mat{X}]^{p \times q}$ which is defined as above and using the similar argument as in the first step, we can deduce that $\mathcal{O}_3$ is nonempty.

\section{Proof of Proposition \ref{testisolated}}
\label{sec:prooftestisolated}
Reference~\cite{BaHaPeSo09} gives an algorithm to compute the dimension of $V(\mathbf{f})$ at $\mathbf{x}$, but its complexity is unclear to us, as it relies on linear algebra with matrices of potentially large size.
Instead, we use an adaptation of a prior result by Mourrain~\cite{Mourrain97}, which allows us to control the size of the matrices we handle. We only give detailed proofs for new ingredients that are specific to our context, a key difference being the cost analysis in the straight-line program model: Mourrain's original result depends on the number of monomials appearing when we expand $\mathbf{f}$, which would be too high for the applications we will make of this result.

We assume that $\mathbf{x} = 0$; this is done by replacing $\mathbf{f}$ by the polynomials $\mathbf{f}(\mat{X}+\mathbf{x})$, which have complexity of evaluation $\mathcal{E}'= \mathcal{E}+n$.  The basis of our algorithm is the following remark.
\begin{Lemma}
Let $I$ be the zero-dimensional ideal $\langle \mathbf{f} \rangle + \mathfrak{m} ^{\mu+1}$, where $\mathfrak{m} = \langle X_1,\dots,X_n\rangle$ is the maximal ideal at the origin. Then, if $0$ is isolated in $V(\mathbf{f})$ if and only if it has multiplicity at most $\mu$ with respect to $I$.
\end{Lemma}
\begin{proof}
This follows from the following result~\cite[Theorem~A.1]{BaHaPeSo09}.  For $k \ge 1$, let $I_k$ be the zero-dimensional ideal $\langle \mathbf{f} \rangle + \mathfrak{m}^{k}$, and let $\nu_k$ be multiplicity of the origin with respect to this ideal. Then, the reference above proves that the sequence $(\nu_k)_{k \ge 1}$ is non-decreasing, and that $0$ is isolated in $V(\mathbf{f})$ if and only if there exists $k\ge 1$ such that $\nu_k=\nu_{k+1}$.
\begin{itemize}
\item If $0$ is isolated in $V(\mathbf{f})$, then by assumption $\mathsf{H}$  its multiplicity with respect to $\langle \mathbf{f} \rangle$ is at most $\mu$, and its multiplicity with respect to $I$ cannot be larger.
\item Otherwise, by the result above, $\nu_{k+1} > \nu_k$ holds for all $k \ge 1$, so that $\nu_k \ge k$ holds for all such $k$ (since $\nu_1=1$). In particular, the multiplicity of the origin with respect to $I$, which is $\nu_{\mu+1}$, is at least $\mu+1$.
    \qedhere
  \end{itemize}
\end{proof}

Hence, we are left with deciding whether the multiplicity of the $\mathfrak{m}$-primary ideal $I$ is at most $\mu$. We do this by following and slightly modifying Mourrain's algorithm for the computation of the orthogonal $I^{\perp}$, that is, the set of $\field$-linear forms $\field[\mat{X}] \to \field$ that vanish on $I$; this is a $\field$-vector space naturally identified with the dual of $\field[\mat{X}]/I$, so it has dimension $m = \mathrm{mult}(0,I)$. 

We do not need to give all details of the algorithm, let alone proof of correctness; we just mention the key ingredients for the cost analysis in our setting. A linear form $\beta: \field[\mat{X}] \to \field$ that vanishes on $I$ must vanish on all monomials, except a finite number (since all monomials, except a finite number, belong to $I$); a natural way to represent such a linear form would then be as the finite generating series $\sum_{\mathbf{\alpha} \in \mathbb{N}^n}\beta(X_1^{\alpha_1}\cdots X_n^{\alpha_n}) d_1^{\alpha_1}\cdots d_n^{\alpha_n}$, for some new variables $d_1,\dots,d_n$; however the number of non-zero coefficients in such a sum cannot be bounded polynomially in $n,\mu$.

Hence, the algorithm uses another way to represent the elements in $I^{\perp}$, by means of {\em multiplication matrices}. An important feature of $I^{\perp}$ is that it admits the structure of a $\field[\mat{X}]$-module: for $k$ in $\{1,\dots,n\}$ and $\beta$ in $I^{\perp}$, the $\field$-linear form $X_k \cdot \beta: f \mapsto
\beta(X_k f)$ is easily seen to still be in $I^{\perp}$.  In particular, if $\bm{\beta}=(\beta_1,\dots,\beta_m)$ is a $\field$-basis of $I^{\perp}$, then for all $k$ as above, and all $i$ in $\{1,\dots,m\}$, $Y_k \cdot \beta_i$ is a linear combination of $\beta_1,\dots,\beta_m$. Mourrain's algorithm computes a
basis $\bm{\beta} = (\beta_1,\dots,\beta_m)$ with the following features:
\begin{itemize}
\item for $i$ in $\{1,\dots,m\}$ and $k$ in $\{1,\dots,n\}$, we have
  $X_k \cdot \beta_i=\sum_{0 \le j < i} \lambda^{(k)}_{i,j} \beta_j$
  (hence $\lambda^{(k)}_{i,j}$ may be non-zero 
  only for $j<i$)
\item $\beta_1$ is the evaluation at $0$, $f \mapsto f(0)$
\item for $i$ in $\{2,\dots,m\}$, $\beta_i(1)=0$.
\end{itemize}
The following lemma shows that the coefficients $(\lambda^{(k)}_{i,j})$ are sufficient to evaluate  the linear forms $\beta_i$ at $f$ in $\field[\mat{X}]$. More precisely, knowing only their values for $j < i \le s$, for any $s \le m$, allows us to evaluate $\beta_1,\dots,\beta_s$ at such an $f$. The following lemma follows~\cite{Mourrain97} in its description of the matrices $\mat{M}_{k,s}$; the (rather straightforward) complexity analysis in the straight-line program model is new.
\begin{Lemma}\label{lemma:evalbeta} Let $s$ be in $1,\dots,m$, and suppose that the coefficients $\lambda^{(k)}_{i,j}$ are known for $i=1,\dots,s$, $j=0,\dots,i-1$ and $k=1,\dots,n$. Given a straight-line program $\Gamma$ of length $L$ that computes $\mathbf{h}=(h_1,\dots,h_R)$, one can compute $\beta_i(h_r)$, for all $i=1,\dots,s$ and $r=1 \dots,R$, using $(s\,L)^{O(1)}$ operations.
\end{Lemma}
\begin{proof}
By definition, for $h$ in $\field[\mat{X}]$  and $k=1,\dots,n$, the following equality holds:
\[
 \begin{bmatrix}
    \beta_1(X_k h)\\
    \vdots\\
    \beta_s(X_k h)
  \end{bmatrix}=
\mat{M}_{k,s}
  \begin{bmatrix}
    \beta_1(h)\\
    \vdots\\
    \beta_s(h)
  \end{bmatrix},
\quad\text{with}\quad
\mat{M}_{k,s}= \begin{bmatrix}
    \lambda^{(k)}_{1,1} & \cdots & \lambda^{(k)}_{s,1}\\
    \vdots && \vdots \\
    \lambda^{(k)}_{1,s} & \cdots & \lambda^{(k)}_{s,s}
  \end{bmatrix}.
\]
Remark that the matrices $\mat{M}_{k,s}$ all commute. Indeed, for any $k,k'$ in $\{1,\dots,n\}$, and $h$ as above, the relation above implies that 
\[
\Delta_{k,k',s}
  \begin{bmatrix}
    \beta_1(h)\\
    \vdots\\
    \beta_s(h)
  \end{bmatrix} =
  \begin{bmatrix}
0\\ \vdots \\ 0 
  \end{bmatrix},
\] where $\Delta_{k,k',s} = \mat{M}_{k,s}\mat{M}_{k',s}-\mat{M}_{k',s}\mat{M}_{k,s}.$ Because the linear forms $\beta_1,\dots,\beta_s$ are linearly independent, this implies that all rows of $\Delta_{k,k',s}$ must be zero, as claimed. We then deduce that for any polynomial $h$ in $\field[\mat{X}]$, we have the equality
\[ \begin{bmatrix}
    \beta_1(h)\\
    \vdots\\
    \beta_s(h)
  \end{bmatrix} =
h(\mat{M}_{1,s},\dots,\mat{M}_{N,s})   \begin{bmatrix}
    \beta_1(1)\\
    \vdots\\
    \beta_s(1)
  \end{bmatrix} \]
On the other hand, our assumptions imply that the sequence $(\beta_1(1),\dots,\beta_s(1))$ is simply $(1,0,\dots,0)$. To prove the claim, note that the evaluations $h_1(\mat{M}_{1,s},\dots,\mat{M}_{n,s}),\dots,h_R(\mat{M}_{1,s},\dots,\mat{M}_{n,s})$ can be computed using the straight-line program $\Gamma$ in $(s\,L)^{O(1)}$ operations.
\end{proof}
Mourrain's alorithm proceeds in an iterative manner, starting from $\bm{\beta}^{(1)}=(\beta_{1})$ (and setting $e_1=1$), and computing successively $\bm{\beta}^{(2)}=(\beta_{e_1+1},\dots,\beta_{e_2})$, $\bm{\beta}^{(3)}=(\beta_{e_2+1},\dots,\beta_{e_3})$, \dots for some integers $e_1 \le e_2 \le e_3 \dots$. Mourrain's algorithm stops when $e_{\ell+1}=e_{\ell}$, in which case $\beta_1,\dots,\beta_{e_\ell}$ is a $\field$-basis of $I^\perp$, and $e_\ell=\mult(0,I)$. In our case, we are not interested in computing this multiplicity, but only in
deciding whether it is less than or equal to the parameter $\mu$. We do it as follows: assume that we have
computed $\bm{\beta}^{(1)},\bm{\beta}^{(2)},\dots,\bm{\beta}^{(\ell)}$, together with the corresponding integers $e_1,e_2,\dots,e_\ell$, with $e_1 < \cdots < e_\ell \le \mu$. We compute $\bm{\beta}^{(\ell+1)}$ and $e_{\ell+1}$, and continue according to the following:
\begin{itemize}
\item if $e_{\ell+1}=e_{\ell}$, we conclude that the multiplicity $\mult(0,I)$ is $e_\ell \le \mu$; we stop the algorithm;
\item if $e_{\ell+1} > \mu$, we conclude that the multiplicity is greater than $\mu$; we stop the algorithm;
\item else, when $e_\ell < e_{\ell+1} \le \mu$, we do another loop.
\end{itemize}
Because the $e_\ell$'s are an increasing sequence of integers, they satisfy $e_\ell \ge \ell$; hence, every time we enter the loop above we have $\ell \le \mu$. To finish the analysis of the algorithm, it remains to explain how to compute $\bm{\beta}^{(\ell+1)}$ from $(\bm{\beta}^{(1)},\bm{\beta}^{(2)},\dots,\bm{\beta}^{(\ell)})=(\beta_{1},\dots,\beta_{e_\ell})$.

As per our description above, at any step of the algorithm, $\beta_{1},\dots,\beta_{e_\ell}$ are represented by means of the coefficients $\lambda^{(k)}_{i,j}$, for $0 \le j < i \le e_{\ell}$ and $1 \le k \le n$.  At step $\ell$, Mourrain's algorithm solves a homogeneous linear system $S_\ell$ with $n(n-1) e_\ell/2+M'$ equations and $n.e_\ell$ unknowns, where $M'$ is the number of generators of the ideal $I= \langle \mathbf{f} \rangle + \mathfrak{m}^{\mu+1}$. Remark that $M'$ is not polynomial in $\mu$ and $n$, so the size of $S_\ell$ is {\em a priori} too large to fit our cost bound; we will explain below how to resolve this issue.

The nullspace dimension of this linear system gives us the cardinality $e_{\ell+1}-e_{\ell}$ of $\bm{\beta}^{(\ell +1)}$. Similarly, the coordinates of the $e_{\ell+1}-e_{\ell}$ vectors in a nullspace basis are precisely
the coefficients $\lambda^{(k)}_{i,j}$ for $i=e_{\ell}+1,\dots,e_{\ell+1}$, $j=1,\dots,e_\ell$ and $k=1,\dots,n$
(we have $\lambda^{(k)}_{i,j}=0$ for $j=e_{\ell}+1,\dots,i-1$). For all $\ell \ge 2$, all linear forms $\beta$ in $\bm{\beta}^{(\ell)}$ are such that for all $k$ in $\{1,\dots,n\}$, $X_k \cdot \beta$ belongs to the span of $\bm{\beta}^{(1)},\dots,\bm{\beta}^{(\ell-1)}$; in particular, a quick induction shows that all linear forms in $\bm{\beta}^{(1)},\dots,\bm{\beta}^{(\ell)}$ vanish on all monomials of degree at least $\ell$.

There remains the question of setting up the system $S_\ell$. For $k$ in $\{1,\dots,n\}$ and a $\field$-linear form $\beta$, we denote by $X_k^{-1} \cdot \beta$ the $\field$-linear form defined as follows:
\begin{itemize}
\item $(X_k^{-1} \cdot \beta)(X_k f) = \beta(f)$ for all $f$ in $\field[\mat{X}]$,
\item $(X_k^{-1} \cdot \beta)(f)=0$ if $f\in \field[\mat{X}]$ does not depend on $X_k$.
\end{itemize}
In other words, $(X_k^{-1} \cdot \beta)(f)=\beta(\delta_k(f))$ holds for all $f$, where $\delta_k:\field[\mat{X}] \to \field[\mat{X}]$ is the $k$th divided difference operator
\[f \mapsto \frac{f(X_1,\dots,X_n)-f(X_1,\dots,X_{k-1},0,X_{k+1},\dots,X_n)}{X_k}.\]
One verifies that, as the notation suggests, $X_k \cdot (X_k^{-1} \cdot \beta)$ is equal to $\beta$. This being said, we can then describe what the entries of $S_\ell$ are:
\begin{itemize}
\item the first $n(n-1) e_\ell/2$ equations involve only the coefficients $\lambda^{(k)}_{i,j}$ previously computed (we refer to~\cite[Section~4.4]{Mourrain97} for details of how exactly  these entries are distributed in $S_\ell$, as we do not need such details here).
\item each of the other $M'$ equations has coefficient vector
\[c_f = \big (\
 (X_k^{-1} \cdot \beta_1)(f(X_1,\dots,X_k,0,\dots,0)),\dots,\ (X_k^{-1} \cdot \beta_{e_\ell})(f(X_1,\dots,X_k,0,\dots,0))\
\big )_{1 \le k \le n},\]
where $f$ is a generator of $I=\langle \f \rangle +\m^{\mu+1}$.
\end{itemize}
We claim that only those equations corresponding to generators $f_1,\dots,f_M$ of the input system $\f$ are useful, as all others are identically zero 

We pointed out above that any linear form $\beta_i$ in $\beta_1,\dots,\beta_{e_\ell}$ vanishes on all monomials of degree at least $\ell$. Since we saw that we must have $\ell \le \mu$, all $\beta_i$ as above vanish on monomials of degree $\mu$; this implies that $X_k^{-1}\cdot \beta_i$ vanishes on all monomials of degree $\mu+1$. The generators $f$ of $\m^{\mu+1}$ have degree $\mu+1$, and for any such $f$, $f(X_1,\dots,X_k,0,\dots,0)$ is either zero, or of degree $\mu+1$ as well. Hence, for any $k$, $\beta_i$ in $\beta_1,\dots,\beta_{e_\ell}$ and $f$ as above, $(X_k^{-1} \cdot \beta_i)(f(X_1,\dots,X_k,0,\dots,0))$ vanishes. This implies that the vector $c_f$ is identically zero for such an $f$, and that the corresponding equation can be discarded.

Altogether, as claimed above, we see that we have to compute the values
\[(X_k^{-1} \cdot \beta_i)(f_j(X_1,\dots,X_k,0,\dots,0)),\] for $k=1,\dots,n$, $i=1,\dots,e_\ell$ and $j=1,\dots,M$.  Fixing $k$, we let $\f_k = (f_{j,k})_{1 \le j \le M}$, where $f_{j,k}$ is the polynomial $f_j(X_1,\dots,X_k,0,\dots,0)$; note that the system $\f_k$ can be computed by a straight-line program of length $\mathcal{E}'=\mathcal{E}+n$. Then, applying the following lemma with $s=e_\ell \le \mu$ and $\h = \f_k$, we deduce that the values $(X_k^{-1} \cdot \beta_i)(f_j(X_1,\dots,X_k,0,\dots,0))$, for $k$ fixed, can be computed in time $(\mu\,\mathcal{E}\,n)^{\mathcal{O}(1)}$.

\begin{Lemma} Let $s$ be in $1,\dots,m$, and suppose that the coefficients $\lambda^{(k)}_{i,j}$ are known for $i=1,\dots,s$, $j=0,\dots,i-1$ and $k=1,\dots,n$. Given a straight-line program $\Gamma$ of length $L$ that computes $\h=(h_1,\dots,h_R)$ and given $k$ in $\{1,\dots,n\}$, one can compute $(X_k^{-1}\cdot \beta_i)(h_r)$, for all $i=1,\dots,s$ and $r=1,\dots,R$, using $(s\,L\,N)^{O(1)}$ operations in $\field$.
\end{Lemma}
\begin{proof} In view of the formula $(X_k^{-1} \cdot \beta)(f)=\beta(\delta_k(f))$, and of Lemma \ref{lemma:evalbeta}, it is enough to prove the existence of a straight-line program of length $\mathcal{O}(L)$ that computes $(\delta_k(h_1),\dots,\delta_k(h_R))$.

To do this, we replace all polynomials $\gamma_{-n+1},\dots,\gamma_L$ computed by $\Gamma$ by terms $\lambda_{-n+1},\dots,\lambda_L$ and $\mu_{-n+1},\dots,\mu_L$, with $\lambda_\ell=\gamma_\ell(X_1,\dots,X_{k-1},0,X_{k+1},\dots,X_N)$ and $\mu_\ell$ in $\field[\mat{X}]$ such that $\gamma_\ell= \lambda_\ell+X_k \mu_\ell$ holds for all $\ell$, so that in particular
  $\mu_\ell=\delta_k(\gamma_\ell)$.  To compute $\lambda_\ell$ and
  $\mu_\ell$, assuming all previous $\lambda_{\ell'}$ and
  $\mu_{\ell'}$ are known, we proceed as follows:
  \begin{itemize}
  \item if $\gamma_\ell=X_k$, we set $\lambda_\ell=0$ and $\mu_\ell=1$;
  \item if $\gamma_\ell=X_{k'}$, with $k' \ne k$, we set $\lambda_\ell=X_{k'}$ and $\mu_\ell=0$;
  \item if $\gamma_\ell =c_\ell$, with $c_\ell \in \field$,
    then we set $\lambda_\ell=c_\ell$ and  $\mu_\ell=0$;
  \item if $\gamma_\ell = \gamma_{a_\ell} \pm \gamma_{b_\ell}$,
    for some indices $a_\ell,b_\ell < \ell$, 
    then we set $\lambda_\ell=\lambda_{a_\ell}\pm\lambda_{b_\ell}$
    and $\mu_\ell=\mu_{a_\ell}\pm\mu_{b_\ell}$;
\item if $\gamma_\ell = \gamma_{a_\ell} \gamma_{b_\ell}$,
      for some indices $a_\ell,b_\ell < \ell$,
    then we set $\lambda_\ell=\lambda_{a_\ell} \lambda_{b_\ell}$
    and $$\mu_\ell=
\lambda_{a_\ell} \mu_{b_\ell}
+
\mu_{a_\ell} \lambda_{b_\ell}
+
X_k\mu_{a_\ell} \mu_{b_\ell}.$$
\end{itemize}
One verifies that in all cases, the relation $\gamma_\ell= \lambda_\ell+X_k \mu_\ell$ still holds. Since the previous construction allows us to compute $\lambda_\ell$ and $\mu_\ell$ in $\mathcal{O}(1)$ operations from the knowledge of all previous $\lambda_{\ell'}$ and $\mu_{\ell'}$, we deduce that all $\lambda_\ell$ and $\mu_\ell$,
for $\ell=-n+1,\dots,L$, can be computed by a straight-line program of length $\mathcal{O}(L+n)$.
\end{proof}

Taking all values of $k$ into account, we see that we can compute all entries we need to set up the linear system $S_\ell$ using $(\mu\,\mathcal{E}\,n)^{\bigO{1}}$ operations in $\field$. After discarding the useless equations described above, the number of equations and unknowns in the system $S_\ell$ is polynomial in $n$, $M$ and $e_\ell$; since we saw that $n \le M$ and $e_\ell \le \mu$, this implies that we can find a nullspace basis of it in time $(\mu\,M)^{\bigO{1}}$. 

Altogether, the time spend to find $\bm{\beta}^{(\ell+1)}$ from $(\bm{\beta}^{(1)},\bm{\beta}^{(2)},\dots,\bm{\beta}^{(\ell)})=(\beta_{1},\dots,\beta_{e_\ell})$ is polynomial in $\mu\,\mathcal{E}\,M$. Since we saw that we do at most $\mu$ such loops, the cumulated time remains polynomial in $\mu\,\mathcal{E}\,M$, and Proposition~\ref{testisolated} is proved.
\section{Proof of Proposition \ref{degree_fiber}}
\label{sec:proofdegree}
We recall here $J=Q_1 \cap \cdots \cap Q_r$ is an irredundant primary decomposition of $J$ in $\bar{\field}[T,\mat{X}]$ and $P_1,\dots,P_r$ are the associated primes. For some $s \leq r$, let $P_1, \ldots, P_s$ be the minimal primes so that $V(P_1), \ldots, V(P_s)$ are the irreducible components of $V(J)$ in $\bar{\field}^{n+1}$. As from ${\sf H}_1$, these irreducible components all have dimension at least one. Let $t \leq s$ be such that $V(P_1),\dots,V(P_t)$ are the irreducible components of $V(J)$ of dimension one whose image by $\pi_T: (\tau,x_1,\dots,x_N) \mapsto \tau$ is dense in $\field$.
\begin{Lemma}\label{lemma:vPi}
  Let $\tau$ be in $\bar{\field}$ and let ${\bf x} \in \bar{\field}{}^n$ be an isolated solution of the system $\h_\tau$. Then, $(\tau,{\bf x})$ belongs to $V(P_i)$ for at least one index $i$ in $\{1,\dots,t\}$, and does not belong to $V(P_i)$ for any index $i$ in $\{t+1,\dots,r\}$.
\end{Lemma}
\begin{proof}
 Because $(\tau,{\bf x})$ cancels $\h$, it belongs at least to one of
  $V(P_1),\dots,V(P_r)$. It remains to rule out the possibility that
  $(\tau,\y)$ belongs to $V(P_i)$ for some index $i$ in
  $\{t+1,\dots,r\}$.
  
We first deal with indices $i$ in $\{t+1,\dots,s\}$. These are those
  primary components with minimal associated primes $P_i$ that either
  have dimension at least two, or have dimension one but whose image
  by $\pi$ is a single point. In both cases, all irreducible
  components of the intersection $V(P_i)\cap V(T-\tau)$ have dimension
  at least one. Since ${\bf x}$ is isolated in $V(\h_\tau)$, $(\tau,{\bf x})$ is
  isolated in $V(\h)\cap V(T-\tau)$, so it cannot belong to
  $V(P_i)\cap V(T-\tau)$ for any $i$ in $\{t+1,\dots,s\}$.
  
  
  We conclude by proving that $(\tau,{\bf x})$ does not belong to $V(P_i)$,
  for any of the embedded primes $P_{s+1},\dots,P_r$. We proceed by
  contradiction, assuming for definiteness that $(\tau,{\bf x})$ belongs to
  $V(P_{s+1})$. Because $P_{s+1}$ is an embedded prime, $V(P_{s+1})$
  is contained in (at least) one of $V(P_1),\dots,V(P_s)$. In view of
  the previous paragraph, it cannot be one of
  $V(P_{t+1}),\dots,V(P_s)$.  Now, all of $V(P_1),\dots,V(P_t)$ have
  dimension one, so $V(P_{s+1})$ has dimension zero (so it is the point $\{(\tau,{\bf x})\}$). For the same
  reason, if $(\tau,{\bf x})$ belonged to another $V(P_i)$, for some $i >
  s+1$, $V(P_i)$ would also be zero-dimensional, and thus equal to $\{(\tau,{\bf x})\}$; as a result, $V(P_i)$
  would be equal to $V(P_{s+1})$, and this would contradict the
  irredundancy of our decomposition.
  
   To summarize, $(\tau,{\bf x})$ belongs to $V(P_{s+1})$, together with
  $V(P_i)$ for some indices $P_i$ in $\{1,\dots,t\}$ (say
  $P_1,\dots,P_u$, up to reordering, for some $u \ge 1$), and avoids
  all other associated primes.  Let us localize the decomposition
  $J=Q_1 \cap \cdots \cap Q_r$ at
  $P_{s+1}$. By~\cite[Proposition~4.9]{AtMc},
  $J_{P_{s+1}}={Q_1}_{P_{s+1}} \cap \cdots \cap {Q_u}_{P_{s+1}}\cap
  {Q_{s+1}}_{P_{s+1}}$ is an irredundant primary decomposition of
  $J_{P_{s+1}}$ in $\bar{\field},\mat{X}]_{P_{s+1}}$; the minimal primes are
  ${P_1}_{P_{s+1}},\dots,{P_u}_{P_{s+1}}$.
  
  By Corollary~4 p.24 in~\cite{Matsumura86}, for any prime
  ${P_i}_{P_{s+1}}$, $i=1,\dots,u$ or $i=s+1$, the localization of
  $\bar{\field}[T,\mat{X}]_{P_{s+1}}$ at ${P_i}_{P_{s+1}}$ is equal to
  $\bar{\field}[T,\mat{X}]_{P_{i}}$. In particular, the height of ${P_i}_{P_{s+1}}$
  in $\bar{\field}[T,\mat{X}]_{P_{s+1}}$ is equal to that of $P_i$ in
  $\bar{\field}[T,\mat{X}]_{P_{i}}$, that is, $n$ if $i=1,\dots,u$, since then
  $V(P_i)$ has dimension $1$, or $n+1$ if $i=s+1$. Since $u \ge 1$,
  this proves that $J_{P_{s+1}}$ has height $n$. As a result, ${\sf
    H}_2$ implies that $J_{P_{s+1}}$ is unmixed, a contradiction.
\end{proof}

For $\tau$ in $\bar{\field}$, we denote by $J_\tau \subset \bar{\field}[T,\mat{X}]$ the ideal $J + \langle T-\tau \rangle$, and similarly for $J'_\tau$ and $ J''_\tau$.

\begin{Lemma}\label{lemma:JJprime}
  Let $\tau$ and ${\bf x}$ be as in Lemma~\ref{lemma:vPi}. Then, the
  multiplicities of the ideals $J_\tau$ and $J'_\tau$ at $(\tau,{\bf x})$
  are the same.
\end{Lemma}
\begin{proof}
  Without loss of generality, assume that $\tau=0 \in \bar{\field}$ and
  ${\bf x}=0 \in \bar{\field}{}^N$. We start from the equality $J=J' \cap J''$,
  which holds in $\bar{\field}[T, \mat{X}]$, and we see it in $\bar{\field}[T, \mat{X}]$.  The
  previous lemma implies that there exists a polynomial in $J''$ that
  does not vanish at $(\tau,{\bf x})=0 \in \bar{\field}{}^{n+1}$.  This polynomial
  is a unit in $\bar{\field}[T, \mat{X}]$, which implies that the extension of
  $J''$ in $\bar{\field}[T, \mat{X}]$ is the trivial ideal $\langle 1 \rangle$, and
  finally that the equality of extended ideals $J=J'$ holds in
  $\bar{\field}[T, \mat{X}]$. This implies the equality $J+\langle T \rangle
  =J'+\langle T \rangle $ in $\bar{\field}[T, \mat{X}]$, and the conclusion
  follows.
\end{proof}

Our goal is now to give a bound on the sum of the multiplicites of
$\h_\tau$ at all its isolated roots, for any $\tau$ in $\bar{\field}$.

\begin{Lemma}
  The ideal $\frak{J}'$ has dimension zero and $V(\frak{J}') \subset
  \overline{\field(T)}{}^n$ is the set of isolated solutions of
  $V(\frak{J}) \subset \overline{\field(T)}{}^n$.
\end{Lemma}
\begin{proof}
 From the equality $J=J' \cap J''$ and Corollary~3.4 in~\cite{AtMc},
 we deduce that $\frak{J}=\frak{J'} \cap \frak{J''}$; the properties
 of $J'$ (the irreducible components of $V(J')$ are precisely those
 irreducible components of $V(J)$ that have dimension one and with a
 dense image by $\pi_T$) imply our claim.
\end{proof}

Let us write $c=\dim_{\bar{\field}(T)}(\bar{\field}(T)[\mat{X}]/{\frak J}')$.  The
following lemma relates this quantity to the multiplicities of the
solutions in any fiber $\h_\tau$. This proves the first statement
in Proposition~\ref{degree_fiber}.

\begin{Lemma}\label{lemma:19}
  Let $\tau$ be in $\bar{\field}$. The sum of the multiplicities of the
  isolated solutions of $\h_\tau$ is at most equal to $c$.
\end{Lemma}
\begin{proof}
 The sum in the lemma is also the sum of the multiplicities of the
  ideal $J_\tau$ at all $(\tau,{\bf x})$, for ${\bf x}$ an isolated solution of
  $\h_\tau$.  By Lemma~\ref{lemma:JJprime}, this is also the sum of
  the multiplicities of $J'_\tau$ at all $(\tau,{\bf x})$, for ${\bf x}$ an
  isolated solution of $\h_\tau$. We prove below that the sum of the
  multiplicities of $J'_\tau$ at all $(\tau,{\bf x})$, for ${\bf x}$ such that
  $(\tau,{\bf x})$ cancels $J'_\tau$, is at most $c$; this will be enough
  to conclude (for any isolated solution ${\bf x}$ of $\h_\tau$,
  $(\tau, {\bf x})$ is a root of $J'_\tau$, though the converse may not be
  true).
  
  Let $m_1,\dots,m_\mu$ be monomials that form a $\bar{\field}$-basis of
  $\bar{\field}[T, \mat{X}]/J'_\tau$; since $T-\tau$ is in $J'_\tau$, these
  monomials can be assumed not to involve $T$.  We will prove that
  they are still $\bar{\field}(T)$-linearly independent in
  $\bar{\field}(T)[\mat{X}]/{\frak J}'$; this will imply that $\mu \le c$,
  and finish the proof of the first statement.
  
   Suppose that there exists a linear combination $A_1 m_1 + \cdots +
  A_\mu m_\mu$ in ${\frak J}'$, with all $A_i$'s in $\bar{\field}(T)$, not
  all of them zero. Thus, we have an equality $a_1/d_1\, m_1 + \cdots
  + a_\mu/d_\mu\, m_\mu = a/d$, with $a_1,\dots,a_\mu$ and
  $d,d_1,\dots,d_\mu$ in $\bar{\field}[T]$ and $a$ in the ideal
  $J'$. Clearing denominators, we obtain a relation of the form $b_1
  m_1 +\cdots+ b_\mu m_\mu \in J'$, with not all $b_i$'s zero. Let
  $(T-\tau)^e$ be the highest power of $T-\tau$ that divides all
  $b_i$'s (this is well-defined, since not all $b_i$'s vanish) so that
  we can rewrite the above as $(T-\tau)^e (c_1 m_1 +\cdots+ c_\mu
  m_\mu) \in J'$, with $c_i=b_i/(T-\tau)^e \in \bar{\field}[T]$ for all $i$.
  In particular, our definition of $e$ implies that the values
  $c_i(\tau)$ are not all zero.
  
  
  Recall that the ideal $J'$ has the form $J'=Q_1 \cap \cdots \cap
  Q_t$. For $i=1,\dots,t$, since $Q_i$ is primary, the membership
  equality $(T-\tau)^e (c_1 m_1 +\cdots +c_\mu m_\mu) \in J'$ implies
  that either $c_1 m_1 +\cdots +c_\mu m_\mu$ or some power
  $(T-\tau)^{ef}$, for some $f > 0$, is in $Q_i$. Since $Q_i$ does not
  contain non-zero polynomials in $\bar{\field}[T]$, $c_1 m_1 +\cdots+ c_\mu
  m_\mu$ belongs to all $Q_i$'s, that is, to $J'$. We can then
  evaluate this relation at $T=\tau$. We saw that the values
  $c_i(\tau)$ do not all vanish on the left, which is a contradiction
  with the independence of the monomials $m_1,\dots,m_\mu$ modulo
  $J'_\tau$.
\end{proof}

To conclude the proof of Proposition~\ref{degree_fiber}, we
consider $\tau \in \bar{\field}$ such that ${\sf G}(\tau)$ holds. Without
loss of generality, we assume that $\tau=0$.

The field of Puiseux series $\bar{\field}\langle\langle T \rangle\rangle$
contains an algebraic closure of $\bar{\field}(T)$; we thus let
$\Phi_1,\dots,\Phi_{c'}$ be the points of $V(\mathfrak{J}')$, with
coordinates taken in $\bar{\field}\langle\langle T \rangle\rangle$. In
particular, we see that $c' \le c$; we prove below that we actually
have $c'=c$.

For a vector $\Phi=(\varphi_1,\dots,\varphi_s)$
with entries in $\bar{\field}\langle\langle T \rangle\rangle$, the valuation $\nu(\Phi)$ is the minimum of the valuations of its exponents. We say that $\Phi$ is {\em
  bounded} if it has non-negative valuation; in this case,
$\lim_0(\Phi)$ is defined as the vector
$(\lim_0(\varphi_1),\dots,\lim_0(\varphi_s))$, with
$\lim_0(\varphi_i)={\rm coeff}(\varphi_i,T^0)$ for all $i$.
\begin{Lemma}
   $\Phi_1,\dots,\Phi_{c'}$ are bounded.
\end{Lemma}
\begin{proof}
  For $i=1,\dots,c'$, write $\Phi_i=1/T^{e_i}
  (\Psi_{i,1},\dots,\Psi_{n})$, for a vector
  $(\Psi_{i,1},\dots,\Psi_{i,n})$ of Puiseux series of valuation
  zero, that is, such that all $\Psi_{i,j}$ are bounded and
  $(\psi_{i,1},\dots,\psi_{i,n})=\lim_0(\Psi_{i,1},\dots,\Psi_{i,n})$
  is nonzero. Hence,
  $e_i=-\nu(\Phi_i)$, and we have to prove that $e_i \le 0$.  By way
  of contradiction, we assume that $e_i > 0$.

  The Puiseux series $\Phi_i$ cancels $h_1,\dots,h_M$. For
  $k=1,\dots,M$, let $h_k^H \in \bar{\field}[T][X_0,\mat{X}]$ be the homogenization
  of $h_k$ with respect to $\mat{X}$. From the equality
  $h_k^H(T^{e_i},\Psi_{i,1},\dots,\Psi_{i,n})=
  T^{e_i}h_k(\Phi_i)$, we deduce that
  $h_k^H(T^{e_i},\Psi_{i,1},\dots,\Psi_{i,n})=0$ for all $k$. We
  can write $h_k = h_{0,k} + T \tilde h_k$, for some polynomial
  $\tilde h_k$ in $\bar{\field}[T,\mat{X}]$, and ${\sf G}_2(0)$ implies that
  $\deg_\mat{X}(\tilde h_k) \le \deg_\mat{X}(h_{0,k})$. As a result, the
  homogenizations (with respect to $\mat{X}$) of $h_{k},h_{0,k}$ and $\tilde
  h_k$ satisfy a relation of the form $h^H_k = h_{0,k}^H +
  X_0^{\delta_k} T \tilde h^H_k$, for some $\delta_k \ge 0$. This
  implies the equality
  $$h_{0,k}^H(T^{e_i},\Psi_{i,1},\dots,\Psi_{i,n}) + T^{\delta_k
    e_i+1}\tilde h_k^H(T^{e_i},\Psi_{i,1},\dots,\Psi_{i,n})=0.$$
  The second term has positive valuation, so that
  $h_{0,k}^H(T^{e_i},\Psi_{i,1},\dots,\Psi_{i,n})$ has positive
  valuation as well. Taking the coefficient of $T^0$, this means 
  that $h_{0,k}^H(0,\psi_{i,1},\dots,\psi_{i,n})=0$ (since $e_i > 0$), which implies 
  that $(\psi_{i,1},\dots,\psi_{i,n})=(0,\dots,0)$, in view of ${\sf G}_3(0)$.
  This however contradicts the definition of $(\psi_{i,1},\dots,\psi_{i,n})$.
\end{proof}

For $i=1,\dots,c'$, we define $\varphi_i =
(\varphi_{i,1},\dots,\varphi_{i,n})$ as $\varphi_i=\lim_0(\Phi_i)\in
\bar{\field}{}^n$. In particular, all $\varphi_i$, $i=1,\dots,c'$, are roots
of $\h_0$.

\begin{Lemma}\label{lemma:Jprimerad}
  The ideal $\frak{J}'$ is radical; equivalently, $c'=c$.
\end{Lemma}

\begin{proof}
 We know that $\frak{J}'$ has dimension zero, so it is enough to prove
 that for $i=1,\dots,c'$, the localization of $\bar{\field}\langle\langle T
 \rangle\rangle[\mat{X}]/\frak{J}'$ at the maximal ideal
 $\mathfrak{m}_{\Phi_i}$ is a field, or equivalently that the
 localization of $\bar{\field}\langle\langle T \rangle\rangle[\mat{X}]/\frak{J}$
 at $\mathfrak{m}_{\Phi_i}$ is a field.  By the Jacobian
 criterion~\cite[Theorem~16.19.b]{Eisenbud95}, this is the case if and
 only if the Jacobian matrix of $\h$ with respect to $\mat{X}$ has full
 rank $n$ at $\Phi_i$. We know that $\varphi_i=\lim_0(\Phi_i)$ is a
 root of $\h_0$, and the Jacobian criterion conversely implies that
 since the ideal $\langle \h_0 \rangle$ is radical (by assumption
 ${\sf G}_1(0)$) and zero-dimensional (by assumption ${\sf G}_3(0)$),
 the Jacobian matrix of $\h_0(\mat{X})=\h(0,\mat{X})$ has full rank $n$. Since
 this matrix is the limit at zero of the Jacobian matrix of $\h$ with
 respect to $\mat{X}$, taken at $\Phi_i$, the latter must have full rank
 $n$, and our claim that $\frak{J}'$ is radical is proved.
\end{proof}

To finish the proof of Proposition~\ref{degree_fiber}, we have to
establish that $V(\h_0)$ consists of exactly $c$ solutions.  Let thus
$d$ be the number of points in $V(\h_0)$.  Since $\langle \h_0
\rangle$ is radical (this is ${\sf G}_1(0)$), Lemma~\ref{lemma:19}
implies that $d \le c$, so we only have to prove that $c \le d$. To
prove this, we prove that for $i,i'$ in $\{1,\dots,c\}$, with $i \ne
i'$, we have $\varphi_i \ne \varphi_{i'}$.

Suppose to the contrary that $\varphi_i = \varphi_{i'}$. We know that
the Jacobian matrix of $\h_0$ has full rank $N$ at $\varphi_i$; up to
reindexing, we assume that rows $1,\dots,n$ correspond to a maximal
nonzero minor. Let $\h'=(h_1,\dots,h_n)$.

Let $m=\nu(\Phi_i-\Phi_{i'})$; since  $\varphi_i = \varphi_{i'}$, we have
$m > 0$. We can thus write $\Phi_i=f + T^m
\delta_i$ and $\Phi_{i'}=f + T^m \delta_{i'}$, for some vectors of
bounded Puiseux series $f, \delta_i, \delta_{i'}$ such that all terms
in $f$ have valuation less than~$m$; in addition, $\lim_0(\delta_i)
\ne \lim_0(\delta_{i'})$. Write the Taylor expansion of $\h'$ at $f$ as
$$\h'(\Phi_i) = \h'(f) + \mathrm{jac}(\h',\mat{X}) T^m \delta_i + T^{2m} r_i =0$$
and
$$\h'(\Phi_{i'}) = \h'(f) + \mathrm{jac}_f(\h',\mat{X}) T^m \delta_{i'} + T^{2m}
r_{i'} =0,$$ for some vectors of bounded Puiseux series $r_i,r_{i'}$.
By subtraction and division by $T^m$, we obtain $\mathrm{jac}(\h',\mat{X})
(\delta_i-\delta_{i'}) = T^m r$, for some vector of bounded Puiseux
series $r$.  Since $\mathrm{jac}(\h',\mat{X})$ is invertible, this further gives
$\delta_i-\delta_{i'} = T^m r'$, where again $r'$ is a vector of
bounded Puiseux series.  However, by construction the left-hand side
has valuation zero, while the right-hand side has positive valuation
(since $m > 0$). Hence, we derived a contradiction to our assumption
that $\varphi_i = \varphi_{i'}$. The proof of Proposition~\ref{degree_fiber} is
complete.

\section{Proof of Lemma~\ref{lemma:Z1}}
\label{sec:proofZ1}
\end{document}